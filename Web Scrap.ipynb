{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home2/htnv73/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home2/htnv73/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "import operator\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "urlweb = \"https://community.dur.ac.uk/hubert.shum/comp42315/publicationfull_year_characteranimation.htm\"\n",
    "page = requests.get(urlweb)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1 Please design and implement the solution to crawl all the unique URLs for the detailed publication pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motion Analysis https://community.dur.ac.uk/hubert.shum/comp42315/publicationfull_year_motionanalysis.htm\n",
      "Interaction Modelling https://community.dur.ac.uk/hubert.shum/comp42315/publicationfull_year_interactionmodelling.htm\n",
      "3D Reconstruction https://community.dur.ac.uk/hubert.shum/comp42315/publicationfull_year_3dreconstruction.htm\n",
      "Action Recognition https://community.dur.ac.uk/hubert.shum/comp42315/publicationfull_year_actionrecognition.htm\n",
      "Surface Modelling https://community.dur.ac.uk/hubert.shum/comp42315/publicationfull_year_surfacemodelling.htm\n",
      "Virtual Reality https://community.dur.ac.uk/hubert.shum/comp42315/publicationfull_year_virtualreality.htm\n",
      "Biometrics https://community.dur.ac.uk/hubert.shum/comp42315/publicationfull_year_biometrics.htm\n",
      "Face Modelling https://community.dur.ac.uk/hubert.shum/comp42315/publicationfull_year_facemodelling.htm\n",
      "Crowd Modelling https://community.dur.ac.uk/hubert.shum/comp42315/publicationfull_year_crowdmodelling.htm\n",
      "Biomedical Engineering https://community.dur.ac.uk/hubert.shum/comp42315/publicationfull_year_biomedicalengineering.htm\n",
      "Hand and Gesture https://community.dur.ac.uk/hubert.shum/comp42315/publicationfull_year_handandgesture.htm\n",
      "Robotics https://community.dur.ac.uk/hubert.shum/comp42315/publicationfull_year_robotics.htm\n",
      "Machine Learning https://community.dur.ac.uk/hubert.shum/comp42315/publicationfull_year_machinelearning.htm\n",
      "Topology Analysis https://community.dur.ac.uk/hubert.shum/comp42315/publicationfull_year_topologyanalysis.htm\n"
     ]
    }
   ],
   "source": [
    "# crawl all the unique URLs for each topic\n",
    "urls = []\n",
    "urls.append(urlweb)\n",
    "Topics = soup.find(\"p\",class_='TextOption') \n",
    "for Link in Topics. find_all(\"a\"): \n",
    "    url = urlweb[:urlweb.rfind(\"/\")+1]+Link[\"href\"]\n",
    "    urls.append(url)\n",
    "    topic = Link.get_text()\n",
    "    print(topic,url) #print the topic and url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_tvcg2021motionsynthesis.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_tcsvt2021motionprediction.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_cag2021reactivemotion.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_cavw2019emotionanalysis.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_mig2019dancesynthesis.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_ace2017dancedj3d.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_d2at2017emotionsynthesis.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_siggraph2016dancegeneration.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_hhm2016depthsensor.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_pg2015latticemodel.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_casa2014variationsynthesis.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_pg2013topologyik.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_cavw2013preparationsynthesis.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_casa2013preparationsynthesis.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_tvcg2012interactionsynthesis.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_vrst2012physicalmodel.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_patent2010interactionpatent.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_thesis2010interactioncharacter.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_casa2009angularmomentum.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_siggraphasia2008interactionpatches.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_i3d2008interactionavatars.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_mig2008interactioncharacters.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_vrst2007interactionsingly.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_sca2006fightinggametree.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_arxiv2006gametree.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_bhi2021cerebralpalsy.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_tvcg2020interactioncomparison.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_access2020infantmovement.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_mig2019motionanalysis.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_dhmp2019kinectindustry.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_tnsre2018gaitdisorder.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_skima2018emotionrecognition.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_ecticon2018gaitanalysis.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_ae2016ergonomicvalidation.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_cag2017visualizationskill.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_ijie2017ergonomicskinect.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_ijhfms2017ergonomicusability.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_mig2016visualizationskill.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_dhm2016ergonomicspose.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_isb2007boxinganalysis.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_icpr2020interactionskeleton.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_mig2013rgbdgames.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_vc2021carsketch.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_3dv2021durlarcar.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_vc2021depthprediction.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_wscg2020depthestimation.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_grapp2020carsketch.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_mig2019carshape.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_mig2019reconstructionhuman.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_mtap2016filteredgraph.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_tvcg2016posturekinect.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_vrst2014posturereconstruction.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_tcyb2013kinectreconstruction.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_skima2012kinectenvironment.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_tip2018actionrecognition.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_tlt2018chineseteaching.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_iahfar2018actionrecognition.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_skima2017falldetection.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_cviu2016postureclassification.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_icra2016actionrecognition.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_mig2016actionfusion.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_vrcai2016clusteringmotion.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_casa2008motionsimilarity.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_icuimc2008repetitivemotion.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_tmm2020meshmetric.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_neurocomputing2020meshsaliency.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_trj2019clothessimulation.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_mig2019humanprior.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_vr2021handstable.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_ismar2021handdenoise.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_casa2020handobject.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_skima2018networkvisualisation.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_hwb2018augmentedreality.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_miru2018handobject.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_vrst2018handmr.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_ijvr2011accelerometermotion.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_tifs2021facezernike.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_eswa2021handverification.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_mtap2021handspoofing.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_access2020unifyreidentification.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_access2020networkprivacy.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_neurocomputing2019faceverification.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_access2019networkfuzzy.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_wscg2019reidentificationperson.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_eswa2021beautificationface.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_icpr2020makeupstyle.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_skima2018facesynthesis.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_simpac2021trajectorypytorch.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_smc2021trajectorycrowd.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_arxiv2021formationcontrol.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_cgf2018multitouchcontrol.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_skima2017crowdabnormal.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_cgf2016crowdsimulation.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_tvcg2014formationcontrol.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_sca2012crowdenvironment.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_aire2020eegfiltering.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_eswa2017eegevolution.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_gws2017balanceboard.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_skima2014retinalanalysis.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_ichms2021autonomousdriving.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_its2020selfdrivingdeeplearning.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_msc2019pathplanning.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_icra2013robotmotion.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_mig2010physicallycharacter.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_jifs2019fuzzyrule.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_tip2017manifoldactivelearning.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_tip2016relevancefeedback.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_fuzz2016experiencefuzzy.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_fuzz2016sparserule.htm\n",
      "https://community.dur.ac.uk/hubert.shum/comp42315/pbl_ukci2016fuzzytsk.htm\n"
     ]
    }
   ],
   "source": [
    "#Crawl each publication url from the links above and delete the duplicate urls\n",
    "publications=[]\n",
    "Filters = []\n",
    "for url in urls:\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    spanTitles = soup.find_all(\"span\",class_=\"TextSmallDefault\")\n",
    "    for spanTitle in spanTitles:\n",
    "        a = spanTitle.find(\"a\",class_=\"LinkButton\")\n",
    "        if a!= None:\n",
    "            publication =urlweb[:urlweb.rfind(\"/\")+1]+ a[\"href\"]\n",
    "            publications.append(publication)\n",
    "            for publication in publications:\n",
    "                if not publication in Filters: #delete the duplicate urls\n",
    "                    print(publication)\n",
    "                    Filters.append(publication)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2 Please design and implement the solution to crawl all the text-based information of each\n",
    "publication from the website, to convert such information into a suitable data format, and to\n",
    "store it in a data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:  Spatio-temporal Manifold Learning for Human Motions via Long-horizon Modeling\n",
      "abstract:  Data-driven modeling of human motions is ubiquitous in computer graphics and computer vision applications, such as synthesizing realistic motions or recognizing actions. Recent research has shown that such problems can be approached by learning a natural motion manifold using deep learning on a large amount data, to address the shortcomings of traditional data-driven approaches. However, previous deep learning methods can be sub-optimal for two reasons. First, the skeletal information has not been fully utilized for feature extraction. Unlike images, it is difficult to define spatial proximity in skeletal motions in the way that deep networks can be applied for feature extraction. Second, motion is time-series data with strong multi-modal temporal correlations between frames. On the one hand, a frame could be followed by several candidate frames leading to different motions; on the other hand, long-range dependencies exist where a number of frames in the beginning correlate to a number of frames later.  Ineffective temporal modeling would either under-estimate the multi-modality and variance, resulting in featureless mean motion or over-estimate them resulting in jittery motions, which is a major source of visual artifacts. In this paper, we propose a new deep network to tackle these challenges by creating a natural motion manifold that is versatile for many applications. The network has a new spatial component for feature extraction. It is also equipped with a new batch prediction model that predicts a large number of frames at once, such that long-term temporally-based objective functions can be employed to correctly learn the motion multi-modality and variances. With our system, long-duration motions can be predicted/synthesized using an open-loop setup where the motion retains the dynamics accurately. It can also be used for denoising corrupted motions and synthesizing new motions with given control signals. We demonstrate that our system can create superior results comparing to existing work in multiple applications.\n",
      "author:  Wang, He and Ho, Edmond S. L. and Shum, Hubert P. H. and Zhu, Zhanxing\n",
      "year:  2021\n",
      "type:  JOUR\n",
      "journal/booktitle:  IEEE Transactions on Visualization and Computer Graphics\n",
      "volume:  27\n",
      "number of pages:  12\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/TVCG.2019.2936810\n",
      "ISSN:   \n",
      "citation:  74\n",
      "Impact Factor:  4.579\n",
      "——————————————————————————————————————\n",
      "title:  A Quadruple Diffusion Convolutional Recurrent Network for Human Motion Prediction\n",
      "abstract:  Recurrent neural network (RNN) has become popular for human motion prediction thanks to its ability to capture temporal dependencies. However, it has limited capacity in modeling the complex spatial relationship in the human skeletal structure. In this work, we present a novel diffusion convolutional recurrent predictor for spatial and temporal movement forecasting, with multi-step random walks traversing bidirectionally along an adaptive graph to model interdependency among body joints. In the temporal domain, existing methods rely on a single forward predictor with the produced motion deflecting to the drift route, which leads to error accumulations over time. We propose to supplement the forward predictor with a forward discriminator to alleviate such motion drift in the long term under adversarial training. The solution is further enhanced by a backward predictor and a backward discriminator to effectively reduce the error, such that the system can also look into the past to improve the prediction at early frames. The two-way spatial diffusion convolutions and two-way temporal predictors together form a quadruple network. Furthermore, we train our framework by modeling the velocity from observed motion dynamics instead of static poses to predict future movements that effectively reduces the discontinuity problem at early prediction. Our method outperforms the state of the arts on both 3D and 2D datasets, including the Human3.6M, CMU Motion Capture and Penn Action datasets. The results also show that our method correctly predicts both high-dynamic and low-dynamic moving trends with less motion drift.\n",
      "author:  Men, Qianhui and Ho, Edmond S. L. and Shum, Hubert P. H. and Leung, Howard\n",
      "year:  2021\n",
      "type:  JOUR\n",
      "journal/booktitle:  IEEE Transactions on Circuits and Systems for Video Technology\n",
      "volume:  31\n",
      "number of pages:  16\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/TCSVT.2020.3038145\n",
      "ISSN:  10518215\n",
      "citation:  13\n",
      "Impact Factor:  4.685\n",
      "——————————————————————————————————————\n",
      "title:  GAN-based Reactive Motion Synthesis with Class-aware Discriminators for Human-human Interaction\n",
      "abstract:  Creating realistic characters that can react to the users' or another character's movement can benefit computer graphics, games and virtual reality hugely. However, synthesizing such reactive motions in human-human interactions is a challenging task due to the many different ways two humans can interact. While there are a number of successful researches in adapting the generative adversarial network (GAN) in synthesizing single human actions, there are very few on modelling human-human interactions. In this paper, we propose a semi-supervised GAN system that synthesizes the reactive motion of a character given the active motion from another character. Our key insights are two-fold. First, to effectively encode the complicated spatial-temporal information of a human motion, we empower the generator with a part-based long short-term memory (LSTM) module, such that the temporal movement of different limbs can be effectively modelled. We further include an attention module such that the temporal significance of the interaction can be learned, which enhances the temporal alignment of the active-reactive motion pair. Second, as the reactive motion of different types of interactions can be significantly different, we introduce a discriminator that not only tells if the generated movement is realistic or not, but also tells the class label of the interaction. This allows the use of such labels in supervising the training of the generator. We experiment with the SBU and the HHOI datasets. The high quality of the synthetic motion demonstrates the effective design of our generator, and the discriminability of the synthesis also demonstrates the strength of our discriminator.\n",
      "author:  Men, Qianhui and Shum, Hubert P. H. and Ho, Edmond S. L. and Leung, Howard\n",
      "year:  2021\n",
      "type:  JOUR\n",
      "journal/booktitle:  Computers and Graphics\n",
      "volume:   \n",
      "number of pages:  12\n",
      "publisher:  Elsevier\n",
      "DOI:  10.1016/j.cag.2021.09.014\n",
      "ISSN:  00978493\n",
      "citation:  0\n",
      "Impact Factor:  1.936\n",
      "——————————————————————————————————————\n",
      "title:  A Generic Framework for Editing and Synthesizing Multimodal Data with Relative Emotion Strength\n",
      "abstract:  Emotion is considered to be a core element in performances [1]. In computer animation, both body motions and facial expressions are two popular mediums for a character to express the emotion. However, there has been limited research in studying how to effectively synthesize these two types of character movements using different levels of emotion strength with intuitive control, which is difficult to be modelled effectively. In this work, we explore a common model that can be used to represent the emotion for the applications of body motions and facial expressions synthesis. Unlike previous work which encode emotions into discrete motion style descriptors, we propose a continuous control indicator called emotion strength, by controlling which a data-driven approach is presented to synthesize motions with fine control over emotions. Rather than interpolating motion features to synthesize new motion as in existing work, our method explicitly learns a model mapping low-level motion features to the emotion strength. Since the motion synthesis model is learned in the training stage, the computation time required for synthesizing motions at run-time is very low. We further demonstrate the generality of our proposed framework by editing 2D face images using relative emotion strength. As a result, our method can be applied to interactive applications such as computer games, image editing tools and virtual reality applications, as well as offline applications such as animation and movie production. \n",
      "author:  Chan, Jacky C. P. and Shum, Hubert P. H. and Wang, He and Yi, Li and Wei, Wei and Ho, Edmond S. L.\n",
      "year:  2019\n",
      "type:  JOUR\n",
      "journal/booktitle:  Computer Animation and Virtual Worlds\n",
      "volume:  30\n",
      "number of pages:  20\n",
      "publisher:  John Wiley and Sons Ltd.\n",
      "DOI:  10.1002/cav.1871\n",
      "ISSN:   \n",
      "citation:  6\n",
      "Impact Factor:  1.020\n",
      "——————————————————————————————————————\n",
      "title:  Automatic Sign Dance Synthesis from Gesture-based Sign Language\n",
      "abstract:  Automatic dance synthesis has become more and more popular due to the increasing demand in computer games and animations. Existing research generates dance motions without much consideration for the context of the music. In reality, professional dancers make choreography according to the lyrics and music features. In this research, we focus on a particular genre of dance known as sign dance, which combines gesture-based sign language with full body dance motion. We propose a system to automatically generate sign dance from a piece of music and its corresponding sign gesture. The core of the system is a Sign Dance Model trained by multiple regression analysis to represent the correlations between sign dance and sign gesture/music, as well as a set of objective functions to evaluate the quality of the sign dance. Our system can be applied to music visualization, allowing people with hearing difficulties to understand and enjoy music. \n",
      "author:  Iwamoto, Naoya and Shum, Hubert P. H. and Asahina, Wakana and Morishima, Shigeo\n",
      "year:  2019\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2019 International Conference on Motion in Games\n",
      "volume:   \n",
      "number of pages:  9\n",
      "publisher:  ACM\n",
      "DOI:  10.1145/3359566.3360069\n",
      "ISSN:  9781450369947\n",
      "citation:  0\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  DanceDJ: A 3D Dance Animation Authoring System for Live Performance\n",
      "abstract:  Dance is an important component of live performance for expressing emotion and presenting visual context. Human dance performances typically require expert knowledge of dance choreography and professional rehearsal, which are too costly for casual entertainment venues and clubs. Recent advancements in character animation and motion synthesis have made it possible to synthesize virtual 3D dance characters in real-time. The major problem in existing systems is a lack of an intuitive interfaces to control the animation for real-time dance controls. We propose a new system called the DanceDJ to solve this problem. Our system consists of two parts. The first part is an underlying motion analysis system that evaluates motion features including dance features such as the postures and movement tempo, as well as audio features such as the music tempo and structure. As a pre-process, given a dancing motion database, our system evaluates the quality of possible timings to connect and switch different dancing motions. During run-time, we propose a control interface that provides visual guidance. We observe that disk jockeys (DJs) effectively control the mixing of music using the DJ controller, and therefore propose a DJ controller for controlling dancing characters. This allows DJs to transfer their skills from music control to dance control using a similar hardware setup. We map different motion control functions onto the DJ controller, and visualize the timing of natural connection points, such that the DJ can effectively govern the synthesized dance motion. We conducted two user experiments to evaluate the user experience and the quality of the dance character. Quantitative analysis shows that our system performs well in both motion control and simulation quality.\n",
      "author:  Iwamoto, Naoya and Kato, Takuya and Shum, Hubert P. H. and Kakitsuka, Ryo and Hara, Kenta and Morishima, Shigeo\n",
      "year:  2017\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2017 International Conference on Advances in Computer Entertainment Technology\n",
      "volume:   \n",
      "number of pages:  18\n",
      "publisher:   \n",
      "DOI:  10.1007/9783319762708_46\n",
      "ISSN:  9783319762708\n",
      "citation:  4\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Synthesizing Motion with Relative Emotion Strength\n",
      "abstract:  With the advancement in motion sensing technology, acquiring high-quality human motions for creating realistic character animation is much easier than before. Since motion data itself is not the main obstacle anymore, more and more effort goes into enhancing the realism of character animation, such as motion styles and control. In this paper, we explore a less studied area: the emotion of motions. Unlike previous work which encode emotions into discrete motion style descriptors, we propose a continuous control indicator called motion strength, by controlling which a data-driven approach is presented to synthesize motions with fine control over emotions. Rather than interpolating motion features to synthesize new motion as in existing work, our method explicitly learns a model mapping low-level motion features to the emotion strength. Since the motion synthesis model is learned in the training stage, the computation time required for synthesizing motions at run-time is very low. As a result, our method can be applied to interactive applications such as computer games and virtual reality applications, as well as offline applications such as animation and movie production.\n",
      "author:  Ho, Edmond S. L. and Shum, Hubert P. H. and Wang, He and Yi, Li\n",
      "year:  2017\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2017 ACM SIGGRAPH Asia Workshop on DataDriven Animation Techniques\n",
      "volume:   \n",
      "number of pages:  8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "publisher:   \n",
      "DOI:   \n",
      "ISSN:   \n",
      "citation:  3\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Automatic Dance Generation System Considering Sign Language Information\n",
      "abstract:  In recent years, thanks to the development of 3DCG animation editing tools (e.g. MikuMikuDance), a lot of 3D character dance animation movies are created by amateur users. However, it is very difficult to create choreography from scratch without any technical knowledge. Shiratori et al. [2006] produced the dance automatic generation system considering rhythm and intensity of dance motions. However, each segment is selected randomly from database, so the generated dance motion has no linguistic or emotional meanings. Takano et al. [2010] produced a human motion generation system considering motion labels. However, they use simple motion labels like 'running' or 'jump', so they cannot generate motions that express emotions. In reality, professional dancers make choreography based on music features or lyrics in music, and express emotion or how they feel in music. In our work, we aim at generating more emotional dance motion easily. Therefore, we use linguistic information in lyrics, and generate dance motion.\n",
      "author:  Asahina, Wakana and Iwamoto, Naoya and Shum, Hubert P. H. and Morishima, Shigeo\n",
      "year:  2016\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2016 ACM SIGGRAPH\n",
      "volume:   \n",
      "number of pages:  2\n",
      "publisher:  ACM\n",
      "DOI:  10.1145/2945078.2945101\n",
      "ISSN:  9781450343718\n",
      "citation:  6\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Depth Sensor based Facial and Body Animation Control\n",
      "abstract:  Depth sensors have become one of the most popular means of generating human facial and posture information in the past decade. By coupling a depth camera and computer vision based recognition algorithms, these sensors can detect human facial and body features in real time. Such a breakthrough has fused many new research directions in animation creation and control, which also has opened up new challenges. In this chapter, we explain how depth sensors obtain human facial and body information. We then discuss on the main challenge on depth sensor-based systems, which is the inaccuracy of the obtained data, and explain how the problem is tackled. Finally, we point out the emerging applications in the field, in which human facial and body feature modeling and understanding is a key research problem.\n",
      "author:  Shen, Yijun and Zhang, Jingtian and Yang, Longzhi and Shum, Hubert P. H.\n",
      "year:  2016\n",
      "type:  CHAP\n",
      "journal/booktitle:  Handbook of Human Motion\n",
      "volume:   \n",
      "number of pages:  16\n",
      "publisher:  Springer International Publishing\n",
      "DOI:  10.1007/9783319308081_71\n",
      "ISSN:  9783319308081\n",
      "citation:  1\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Multi-layer Lattice Model for Real-Time Dynamic Character Deformation\n",
      "abstract:  Due to the recent advancement of computer graphics hardware and software algorithms, deformable characters have become more and more popular in real-time applications such as computer games. While there are mature techniques to generate primary deformation from skeletal movement, simulating realistic and stable secondary deformation such as jiggling of fats remains challenging. On one hand, traditional volumetric approaches such as the finite element method require higher computational cost and are infeasible for limited hardware such as game consoles. On the other hand, while shape matching based simulations can produce plausible deformation in real-time, they suffer from a stiffness problem in which particles either show unrealistic deformation due to high gains, or cannot catch up with the body movement. In this paper, we propose a unified multi-layer lattice model to simulate the primary and secondary deformation of skeleton-driven characters. The core idea is to voxelize the input character mesh into multiple anatomical layers including the bone, muscle, fat and skin. Primary deformation is applied on the bone voxels with lattice-based skinning. The movement of these voxels is propagated to other voxel layers using lattice shape matching simulation, creating a natural secondary deformation. Our multi-layer lattice framework can produce simulation quality comparable to those from other volumetric approaches with a significantly smaller computational cost. It is best to be applied in real-time applications such as console games or interactive animation creation. \n",
      "author:  Iwamoto, Naoya and Shum, Hubert P. H. and Yang, Longzhi and Morishima, Shigeo\n",
      "year:  2015\n",
      "type:  JOUR\n",
      "journal/booktitle:  Computer Graphics Forum\n",
      "volume:  34\n",
      "number of pages:  11\n",
      "publisher:  John Wiley and Sons Ltd.\n",
      "DOI:  10.1111/cgf.12749\n",
      "ISSN:  14678659\n",
      "citation:  14\n",
      "Impact Factor:  2.078\n",
      "——————————————————————————————————————\n",
      "title:  Human Motion Variation Synthesis with Multivariate Gaussian Processes\n",
      "abstract:  Human motion variation synthesis is important for crowd simulation and interactive applications to enhance synthesis quality. In this paper, we propose a novel generative probabilistic model to synthesize variations of human motion. Our key idea is to model the conditional distribution of each joint via a multivariate Gaussian process model, namely semi-parametric latent factor model (SLFM). SLFM can effectively model the correlations between degrees of freedom (DOFs) of joints rather than dealing with each DOF separately as implemented in existing methods. A detailed evaluation is performed to show that the proposed approach can effectively synthesize variations of different types of motions. Motions generated by our method show a richer variation compared with existing ones. Finally, our user study shows that the synthesized motion has a similar level of naturalness to captured human motions. Our method is best applied in computer games and animations to introduce motion variations.\n",
      "author:  Zhou, Liuyang and Shang, Lifeng and Shum, Hubert P. H. and Leung, Howard\n",
      "year:  2014\n",
      "type:  JOUR\n",
      "journal/booktitle:  Computer Animation and Virtual Worlds\n",
      "volume:  25\n",
      "number of pages:  9\n",
      "publisher:  John Wiley and Sons Ltd.\n",
      "DOI:  10.1002/cav.1599\n",
      "ISSN:   \n",
      "citation:  6\n",
      "Impact Factor:  1.020\n",
      "——————————————————————————————————————\n",
      "title:  Topology Aware Data-Driven Inverse Kinematics\n",
      "abstract:  Creating realistic human movement is a time consuming and labour intensive task. The major difficulty is that the user has to edit individual joints while maintaining an overall realistic and collision free posture. Previous research suggests the use of data-driven inverse kinematics, such that one can focus on the control of a few joints, while the system automatically composes a natural posture. However, as a common problem of kinematics synthesis, penetration of body parts is difficult to avoid in complex movements. In this paper, we propose a new data-driven inverse kinematics framework that conserves the topology of the synthesizing postures. Our system monitors and regulates the topology changes using the Gauss Linking Integral (GLI), such that penetration can be efficiently prevented. As a result, complex motions with tight body movements, as well as those involving interaction with external objects, can be simulated with minimal manual intervention. Experimental results show that using our system, the user can create high quality human motion in real-time by controlling a few joints using a mouse or a multi-touch screen. The movement generated is both realistic and penetration free. Our system is best applied for interactive motion design in computer animations and games.\n",
      "author:  Ho, Edmond S. L. and Shum, Hubert P. H. and Cheung, Yiu-ming and Yuen, P. C.\n",
      "year:  2013\n",
      "type:  JOUR\n",
      "journal/booktitle:  Computer Graphics Forum\n",
      "volume:  32\n",
      "number of pages:  10\n",
      "publisher:  John Wiley and Sons Ltd.\n",
      "DOI:  10.1111/cgf.12212\n",
      "ISSN:   \n",
      "citation:  26\n",
      "Impact Factor:  2.078\n",
      "——————————————————————————————————————\n",
      "title:  Natural Preparation Behavior Synthesis\n",
      "abstract:  Humans adjust their movements in advance to prepare for the forthcoming action, resulting in an efficient and smooth transition. However, traditional computer animation approaches such as motion graphs simply concatenates a series of actions without taking into account the following one. In this paper, we propose a new method to produce preparation behaviours using reinforcement learning. As an offline process, the system learns the optimal way to approach a target and prepare for interaction. A scalar value called the level of preparation is introduced, which represents the degree of transition from the initial action to the interacting action. To synthesize the movements of preparation, we propose a customized motion blending scheme based on the level of preparation, which is followed by an optimization framework that adjusts the posture to keep the balance. During run-time, the trained controller drives the character to move to a target with the appropriate level of preparation, resulting in a human-like behaviour. We create scenes in which the character has to move in a complex environment and interacts with objects, such as crawling under and jumping over obstacles while walking. The method is useful not only for computer animation, but also for real-time applications such as computer games, in which the characters need to accomplish a series of tasks in a given environment.\n",
      "author:  Shum, Hubert P. H. and Hoyet, Ludovic and Ho, Edmond S. L. and Komura, Taku and Multon, Franck\n",
      "year:  2013\n",
      "type:  JOUR\n",
      "journal/booktitle:  Computer Animation and Virtual Worlds\n",
      "volume:  25\n",
      "number of pages:  10\n",
      "publisher:  John Wiley and Sons Ltd.\n",
      "DOI:  10.1002/cav.1546\n",
      "ISSN:   \n",
      "citation:  1\n",
      "Impact Factor:  1.020\n",
      "——————————————————————————————————————\n",
      "title:  Preparation Behaviour Synthesis with Reinforcement Learning\n",
      "abstract:  When humans perform a series of motions, they prepare for the next motion in advance so as to enhance the response time of their movements. This kind of preparation behaviour results in a natural and smooth transition of the overall movement. In this paper, we propose a new method to synthesize the behaviour using reinforcement learning. To create preparation movements, we propose a customized motion blending algorithm that is governed by a single numerical value, which we called the level of preparation. During the offline process, the system learns the optimal way to approach a target, as well as the realistic behaviour to prepare for interaction considering the level of preparation. At run-time, the trained controller indicates the character to move to a target with the appropriate level of preparation, resulting in human-like movements. We synthesized scenes in which the character has to move in a complex environment and interact with objects, such as a character crawling under and jumping over obstacles while walking. The method is useful not only for computer animation, but also for real-time applications such as computer games, in which the characters need to accomplish a series of tasks in a given environment.\n",
      "author:  Shum, Hubert P. H. and Hoyet, Ludovic and Ho, Edmond S. L. and Komura, Taku and Multon, Franck\n",
      "year:  2013\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2013 International Conference on Computer Animation and Social Agents\n",
      "volume:   \n",
      "number of pages:  10\n",
      "publisher:  John Wiley and Sons Ltd.\n",
      "DOI:   \n",
      "ISSN:   \n",
      "citation:  2\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:  Simulating Multiple Character Interactions with Collaborative and Adversarial Goals\n",
      "abstract:  This paper proposes a new methodology for synthesizing animations of multiple characters, allowing them to intelligently compete with one another in dense environments, while still satisfying requirements set by an animator. To achieve these two conflicting objectives simultaneously, our method separately evaluates the competition and collaboration of the interactions, integrating the scores to select an action that maximizes both criteria. We extend the idea of min-max search, normally used for strategic games such as chess. Using our method, animators can efficiently produce scenes of dense character interactions such as those in collective sports or martial arts. The method is especially effective for producing animations along story lines, where the characters must follow multiple objectives, while still accommodating geometric and kinematic constraints from the environment.\n",
      "author:  Shum, Hubert P. H. and Komura, Taku and Yamazaki, Shuntaro\n",
      "year:  2012\n",
      "type:  JOUR\n",
      "journal/booktitle:  IEEE Transactions on Visualization and Computer Graphics\n",
      "volume:  18\n",
      "number of pages:  12\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/TVCG.2010.257\n",
      "ISSN:  10772626\n",
      "citation:  68\n",
      "Impact Factor:  4.579\n",
      "——————————————————————————————————————\n",
      "title:  Real-time Physical Modelling of Character Movements with Microsoft Kinect\n",
      "abstract:  With the advancement of motion tracking hardware such as the Microsoft Kinect, synthesizing human-like characters with real-time captured movements becomes increasingly important. Traditional kinematics and dynamics approaches perform sub-optimally when the captured motion is noisy or even incomplete. In this paper, we proposed a unified framework to control physically simulated characters with live captured motion from Kinect. Our framework can synthesize any posture in a physical environment using external forces and torques computed by a PD controller. The major problem of Kinect is the incompleteness of the captured posture, with some degree of freedom (DOF) missing due to occlusions and noises. We propose to search for a best matched posture from a motion database constructed in a dimensionality reduced space, and substitute the missing DOF to the live captured data. Experimental results show that our method can synthesize realistic character movements from noisy captured motion. The proposed algorithm is computationally efficient and can be applied to a wide variety of interactive virtual reality applications such as motion-based gaming, rehabilitation and sport training.\n",
      "author:  Shum, Hubert P. H. and Ho, Edmond S. L.\n",
      "year:  2012\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 18th ACM Symposium on Virtual Reality Software and Technology\n",
      "volume:   \n",
      "number of pages:  8\n",
      "publisher:  ACM\n",
      "DOI:  10.1145/2407336.2407340\n",
      "ISSN:  9781450314695\n",
      "citation:  68\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Manufacturing Video Graphics\n",
      "abstract:  An apparatus comprising: a memory system storing a plurality of sequences, each sequence comprising data for reproducing a different pattern of interactions between a respective plurality of moving characters and storing a combination data structure defining for each sequence connectability of that sequence with other ones of the plurality of sequences; a processor configured to determine pair-wise combination of stored sequences, by selecting sequences for pair-wise combination that are defined as connectable by the stored combination data structure, wherein each pair-wise combination has in common at least one of their respective plurality of moving characters and configured to use determined pair-wise combinations of the stored sequences to produce and output video graphics comprising a series of sequences in which movable characters repetitively interact in different combinations.\n",
      "author:  Komura, Taku and Shum, Hubert P. H.\n",
      "year:  2010\n",
      "type:  GEN\n",
      "journal/booktitle:   \n",
      "volume:   \n",
      "number of pages:   \n",
      "publisher:  WO Patent WO/2010/057897\n",
      "DOI:   \n",
      "ISSN:   \n",
      "citation:  0\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Simulating Interactions Among Multiple Characters\n",
      "abstract:  In this thesis, we attack a challenging problem in the field of character animation: synthesizing interactions among multiple virtual characters in real-time. Although there are heavy demands in the gaming and animation industries, no systemic solution has been proposed due to the difficulties to model the complex behaviors of the characters.We represent the continuous interactions among characters as a discrete Markov Decision Process, and design a general objective function to evaluate the immediate rewards of launching an action. By applying game theory such as tree expansion and min-max search, the optimal actions that benefit the character the most in the future are selected. The simulated characters can interact competitively while achieving the requests from animators cooperatively.Since the interactions between two characters depend on a lot of criteria, it is difficult to exhaustively precompute the optimal actions for all variations of these criteria. We design an off-policy approach that samples and precomputes only meaningful interactions. With the precomputed policy, the optimal movements under different situations can be evaluated in real-time.To simulate the interactions for a large number of characters with minimal computational overhead, we propose a method to precompute short durations of interactions between two characters as connectable patches. The patches are concatenated spatially to generate interactions with multiple characters, and temporally to generate longer interactions. Based on the optional instructions given by the animators, our system automatically applies concatenations to create a huge scene of interacting crowd.We demonstrate our system by creating scenes with high quality interactions. On one hand, our algorithm can automatically generate artistic scenes of interactions such as the fighting scenes in movies that involve hundreds of characters. On the other hand, it can create controllable, intelligent characters that interact with the opponents for real-time applications such as 3D computer games.\n",
      "author:  Shum, Hubert P. H.\n",
      "year:  2010\n",
      "type:  THES\n",
      "journal/booktitle:   \n",
      "volume:   \n",
      "number of pages:  151\n",
      "publisher:  University of Edinburgh\n",
      "DOI:   \n",
      "ISSN:   \n",
      "citation:  1\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Angular Momentum Guided Motion Concatenation\n",
      "abstract:  In this paper, we propose a new method to concatenate two dynamic full-body motions such as punches, kicks and flips by using the angular momentum as a cue. Through the observation of real humans, we have identified two patterns of angular momentum that make the transition of such motions efficient. Based on these observations, we propose a new method to concatenate two full-body motions in a natural manner. Our method is useful for applications where dynamic, full-body motions are required, such as 3D computer games and animations.\n",
      "author:  Shum, Hubert P. H. and Komura, Taku and Yadav, Pranjul\n",
      "year:  2009\n",
      "type:  JOUR\n",
      "journal/booktitle:  Computer Animation and Virtual Worlds\n",
      "volume:  20\n",
      "number of pages:  10\n",
      "publisher:  John Wiley and Sons Ltd.\n",
      "DOI:  10.1002/cav.v20:2/3\n",
      "ISSN:  15464261\n",
      "citation:  11\n",
      "Impact Factor:  1.020\n",
      "——————————————————————————————————————\n",
      "title:  Interaction Patches for Multi-Character Animation\n",
      "abstract:  We propose a data-driven approach to automatically generate a scene where tens to hundreds of characters densely interact with each other. During offline processing, the close interactions between characters are precomputed by expanding a game tree, and these are stored as data structures called interaction patches. Then, during run-time, the system spatio-temporally concatenates the interaction patches to create scenes where a large number of characters closely interact with one another. Using our method, it is possible to automatically or interactively produce animations of crowds interacting with each other in a stylized way. The method can be used for a variety of applications including TV programs, advertisements and movies.\n",
      "author:  Shum, Hubert P. H. and Komura, Taku and Shiraishi, Masashi and Yamazaki, Shuntaro\n",
      "year:  2008\n",
      "type:  JOUR\n",
      "journal/booktitle:  ACM Transactions on Graphics\n",
      "volume:  27\n",
      "number of pages:  8\n",
      "publisher:  ACM\n",
      "DOI:  10.1145/1409060.1409067\n",
      "ISSN:  07300301\n",
      "citation:  125\n",
      "Impact Factor:  5.414\n",
      "——————————————————————————————————————\n",
      "title:  Simulating Interactions of Avatars in High Dimensional State Space\n",
      "abstract:  Efficient computation of strategic movements is essential to control virtual avatars intelligently in computer games and 3D virtual environments. Such a module is needed to control non-player characters (NPCs) to fight, play team sports or move through a mass crowd. Reinforcement learning is an approach to achieve real-time optimal control. However, the huge state space of human interactions makes it difficult to apply existing learning methods to control avatars when they have dense interactions with other characters. In this research, we propose a new methodology to ef?ciently plan the movements of an avatar interacting with another. We make use of the fact that the subspace of meaningful interactions is much smaller than the whole state space of two avatars. We efficiently collect samples by exploring the subspace where dense interactions between the avatars occur and favor samples that have high connectivity with the other samples. Using the collected samples, a ?nite state machine (FSM) called Interaction Graph is composed. At run-time, we compute the optimal action of each avatar by min-max search or dynamic programming on the Interaction Graph. The methodology is applicable to control NPCs in ?ghting and ball-sports games.\n",
      "author:  Shum, Hubert P. H. and Komura, Taku and Yamazaki, Shuntaro\n",
      "year:  2008\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2008 Symposium on Interactive 3D Graphics and Games\n",
      "volume:   \n",
      "number of pages:  8\n",
      "publisher:  ACM\n",
      "DOI:  10.1145/1342250.1342271\n",
      "ISSN:  9781595939838\n",
      "citation:  45\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Simulating Interactions of Characters\n",
      "abstract:  It is difficult to create scenes where multiple characters densely interact with each other. Manually creating the motions of characters is time consuming due to the correlation of the movements between the characters. Capturing the motions of multiple characters is also difficult as it requires a huge amount of post-processing of the data. In this paper, we explain the methods we have proposed to simulate close interactions of characters based on singly captured motions. We propose methods to (1) control characters intelligently to cooperatively/competitively interact with the other characters, and (2) generate movements that include close interactions such as tangling the segments with the others by taking into account the topological relationship of the characters.\n",
      "author:  Komura, Taku and Shum, Hubert P. H. and Ho, Edmond S. L.\n",
      "year:  2008\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the First International Conference on Motion in Games\n",
      "volume:   \n",
      "number of pages:  10\n",
      "publisher:  SpringerVerlag\n",
      "DOI:  10.1007/9783540892205_10\n",
      "ISSN:  9783540892199\n",
      "citation:  0\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Simulating Competitive Interactions using Singly Captured Motions\n",
      "abstract:  It is difficult to create scenes where multiple avatars are fighting / competing with each other. Manually creating the motions of avatars is time consuming due to the correlation of the movements between the avatars. Capturing the motions of multiple avatars is also difficult as it requires a huge amount of post-processing. In this paper, we propose a new method to generate a realistic scene of avatars densely interacting in a competitive environment. The motions of the avatars are considered to be captured individually, which will increase the easiness of obtaining the data. We propose a new algorithm called the temporal expansion approach which maps the continuous time action plan to a discrete space such that turnbased evaluation methods can be used. As a result, many mature algorithms in game such as the min-max search and alpha?beta pruning can be applied.Using our method, avatars will plan their strategies taking into account the reaction of the opponent. Fighting scenes with multiple avatars are generated to demonstrate the effectiveness of our algorithm. The proposed method can also be applied to other kinds of continuous activities that require strategy planning such as sport games.\n",
      "author:  Shum, Hubert P. H. and Komura, Taku and Yamazaki, Shuntaro\n",
      "year:  2007\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2007 ACM symposium on Virtual Reality Software and Technology\n",
      "volume:   \n",
      "number of pages:  8\n",
      "publisher:  ACM\n",
      "DOI:  10.1145/1315184.1315194\n",
      "ISSN:  9781595938633\n",
      "citation:  45\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:  Generating Realistic Fighting Scenes by Game Tree\n",
      "abstract:  Recently, there have been a lot of researches to synthesize/edit the motion of a single avatar in the virtual environment. However, there has not been so much work of simulating continuous interactions of multiple avatars such as fighting. In this paper, we propose a new method to generate a realistic fighting scene based on motion capture data. We propose a new algorithm called the temporal expansion approach which maps the continuous time action plan to a discrete causality space such that turn-based evaluation methods can be used. As a result, it is possible to use many mature algorithms available in strategy games such as the Minimax algorithm and α-β pruning. We also propose a method to generate and use an offense/defense table, which illustrates the spatialtemporal relationship of attacks and dodges, to incorporate tactical maneuvers of defense into the scene. Using our method, avatars will plan their strategies taking into account the reaction of the opponent. Fighting scenes with multiple avatars are generated to demonstrate the effectiveness of our algorithm. The proposed method can also be applied to other kinds of continuous activities that require strategy planning such as sport games.\n",
      "author:  Shum, Hubert P. H. and Komura, Taku\n",
      "year:  2006\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2006 ACM SIGGRAPH/Eurographics Symposium on Computer animation\n",
      "volume:   \n",
      "number of pages:  2\n",
      "publisher:  Eurographics Association\n",
      "DOI:   \n",
      "ISSN:  3905673347\n",
      "citation:  6\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Technical Note: Generating Realistic Fighting Scenes by Game Tree\n",
      "abstract:  Recently, there have been a lot of researches to synthesize/edit the motion of a single avatar in the virtual environment. However, there has not been so much work of simulating continuous interactions of multiple avatars such as fighting. In this paper, we propose a new method to generate a realistic fighting scene based on motion capture data. We propose a new algorithm called the temporal expansion approach which maps the continuous time action plan to a discrete causality space such that turn-based evaluation methods can be used. As a result, it is possible to use many mature algorithms available in strategy games such as the Minimax algorithm and α-β pruning. We also propose a method to generate and use an offense/defense table, which illustrates the spatialtemporal relationship of attacks and dodges, to incorporate tactical maneuvers of defense into the scene. Using our method, avatars will plan their strategies taking into account the reaction of the opponent. Fighting scenes with multiple avatars are generated to demonstrate the effectiveness of our algorithm. The proposed method can also be applied to other kinds of continuous activities that require strategy planning such as sport games.\n",
      "author:  Shum, Hubert P. H. and Komura, Taku\n",
      "year:  2006\n",
      "type:  Preprint\n",
      "journal/booktitle:   \n",
      "volume:   \n",
      "number of pages:  7\n",
      "publisher:   \n",
      "DOI:   \n",
      "ISSN:   \n",
      "citation:  0\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Interpreting Deep Learning based Cerebral Palsy Prediction with Channel Attention\n",
      "abstract:  Early prediction of cerebral palsy is essential as it leads to early treatment and monitoring. Deep learning has shown promising results in biomedical engineering thanks to its capacity of modelling complicated data with its non-linear architecture. However, due to their complex structure, deep learning models are generally not interpretable by humans, making it difficult for clinicians to rely on the findings. In this paper, we propose a channel attention module for deep learning models to predict cerebral palsy from infants' body movements, which highlights the key features (i.e. body joints) the model identifies as important, thereby indicating why certain diagnostic results are found. To highlight the capacity of the deep network in modelling input features, we utilize raw joint positions instead of hand-crafted features. We validate our system with a real-world infant movement dataset. Our proposed channel attention module enables the visualization of the vital joints to this disease that the network considers. Our system achieves 91.67% accuracy, suppressing other state-of-the-art deep learning methods.\n",
      "author:  Zhu, Manli and Men, Qianhui and Ho, Edmond S. L. and Leung, Howard and Shum, Hubert P. H.\n",
      "year:  2021\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2021 IEEEEMBS International Conference on Biomedical and Health Informatics\n",
      "volume:   \n",
      "number of pages:  4\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/BHI50953.2021.9508619\n",
      "ISSN:  26413604\n",
      "citation:  1\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Interaction-based Human Activity Comparison\n",
      "abstract:  Traditional methods for motion comparison consider features from individual characters. However, the semantic meaning of many human activities is usually defined by the interaction between them, such as a high-five interaction of two characters. There is little success in adapting interaction-based features in activity comparison, as they either do not have a fixed topology or are in high dimensional. In this paper, we propose a unified framework for activity comparison from the interaction point of view. Our new metric evaluates the similarity of interaction by adapting the Earth Mover's Distance onto a customized geometric mesh structure that represents spatial-temporal interactions. This allows us to compare different classes of interactions and discover their intrinsic semantic similarity. We created five interaction databases of different natures, covering both two-characters (synthetic and real-people) and character-object interactions, which are open for public uses. We demonstrate how the proposed metric aligns well with the semantic meaning of the interaction. We also apply the metric in interaction retrieval and show how it outperforms existing ones. The proposed method can be used for unsupervised activity detection in monitoring systems and activity retrieval in smart animation systems.\n",
      "author:  Shen, Yijun and Yang, Longzhi and Ho, Edmond S. L. and Shum, Hubert P. H.\n",
      "year:  2020\n",
      "type:  JOUR\n",
      "journal/booktitle:  IEEE Transactions on Visualization and Computer Graphics\n",
      "volume:  26\n",
      "number of pages:  14\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/TVCG.2019.2893247\n",
      "ISSN:   \n",
      "citation:  21\n",
      "Impact Factor:  4.579\n",
      "——————————————————————————————————————\n",
      "title:  Abnormal Infant Movements Classification with Deep Learning on Pose-based Features\n",
      "abstract:  The pursuit of early diagnosis of cerebral palsy has been an active research area with some very promising results using tools such as the General Movements Assessment (GMA). In our previous work, we explored the feasibility of extracting pose-based features from video sequences to automatically classify infant body movement into two categories, normal and abnormal. The classification was based upon the GMA, which was carried out on the video data by an independent expert reviewer. In this paper we extend our previous work by extracting the normalised pose-based feature sets, Histograms of Joint Orientation 2D (HOJO2D) and Histograms of Joint Displacement 2D (HOJD2D), for use in new deep learning architectures. We explore the viability of using these pose-based feature sets for automated classification within a deep learning framework by carrying out extensive experiments on five new deep learning architectures. Experimental results show that the proposed fully connected neural network FCNet performed robustly across different feature sets. Furthermore, the proposed convolutional neural network architectures demonstrated excellent performance in handling features in higher dimensionality. We make the code, extracted features and associated GMA labels publicly available.\n",
      "author:  McCay, Kevin D. and Ho, Edmond S. L. and Shum, Hubert P. H. and Fehringer, Gerhard and Marcroft, Claire and Embleton, Nicholas\n",
      "year:  2020\n",
      "type:  JOUR\n",
      "journal/booktitle:  IEEE Access\n",
      "volume:  8\n",
      "number of pages:  11\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/ACCESS.2020.2980269\n",
      "ISSN:  21693536\n",
      "citation:  21\n",
      "Impact Factor:  3.367\n",
      "——————————————————————————————————————\n",
      "title:  An Interactive Motion Analysis Framework for Diagnosing and Rectifying Potential Injuries Caused Through Resistance Training\n",
      "abstract:  With the rapid increase in individuals participating in resistance training activities, the number of injuries pertaining to these activities has also grown just as aggressively. Diagnosing the causes of injuries and discomfort requires a large amount of resources from highly experienced physiotherapists. In this paper, we propose a new framework to analyse and visualize movement patterns during performance of four major compound lifts. The analysis generated will be used to efficiently determine whether the exercises are being performed correctly, ensuring anatomy remains within its functional range of motion, in order to prevent strain or discomfort that may lead to injury. \n",
      "author:  Hall, Jake and Chan, Jacky C. P. and Shum, Hubert P. H. and Ho, Edmond S. L.\n",
      "year:  2019\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2019 International Conference on Motion in Games\n",
      "volume:   \n",
      "number of pages:  2\n",
      "publisher:  ACM\n",
      "DOI:  10.1145/3359566.3364688\n",
      "ISSN:  9781450369947\n",
      "citation:  0\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Motion Analysis of Work Conditions using Commercial Depth Cameras in Real Industrial Conditions\n",
      "abstract:  Measuring human motion activity in real work condition is challenging as the environment is not controlled, while the worker should perform his/her task without perturbation. Since the early 2010’s affordable and easy-to-use depth cameras, such as the Microsoft Kinect system, have been applied for in-home entertainment for the general public. In this chapter, we evaluate such a system for the use in motion analysis in work conditions, and propose software algorithms to enhance the tracking accuracy. Firstly, we highlighted the high performance of the system when used under the recommended setup without occlusions. However, when the position/orientation of the sensor changes, occlusions may occur and the performance of the system may decrease, making it difficult to be used in real work conditions. Secondly, we propose a software algorithm to adapt the system to challenging conditions with occlusions to enhance the robustness and accuracy. Thirdly, we show that real work condition assessment using such an adapted system leads to similar results comparing with those performed manually by ergonomists. These results show that such adapted systems could be used to support the ergonomists work by providing them with reproducible and objective information about the human movement. It consequently saves ergonomists time and effort and allows them to focus on high-level analysis and actions.\n",
      "author:  Plantard, Pierre and Shum, Hubert P. H. and Multon, Franck\n",
      "year:  2019\n",
      "type:  CHAP\n",
      "journal/booktitle:  DHM and Posturography\n",
      "volume:   \n",
      "number of pages:  10\n",
      "publisher:  Academic Press\n",
      "DOI:  10.1016/B9780128167137.000520\n",
      "ISSN:  9780128167137\n",
      "citation:  0\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:  Automatic Musculoskeletal and Neurological Disorder Diagnosis with Relative Joint Displacement from Human Gait\n",
      "abstract:  Musculoskeletal and neurological disorders are common devastating companions of ageing, leading to a reduction in quality of life and increased mortality. Gait analysis is a popular method for diagnosing these disorders. However, manually analysing the motion data is a labour-intensive task, and the quality of the results depends on the experience of the doctors. In this paper, we propose an automatic framework for classifying musculoskeletal and neurological disorders among older people based on 3D motion data. We also propose two new features to capture the relationship between joints across frames, known as 3D Relative Joint Displacement (3DRJDP) and 6D Symmetric Relative Joint Displacement (6DSymRJDP), such that relative movement between joints can be analyzed. To optimize the classification performance, we adapt feature selection methods to choose an optimal feature set from the raw feature input. Experimental results show that we achieve a classification accuracy of 84.29% using the proposed relative joint features, outperforming existing features that focus on the movement of individual joints. Considering the limited open motion database for gait analysis focusing on such disorders, we construct a comprehensive, openly accessible 3D full-body motion database from 45 subjects.\n",
      "author:  Rueangsirarak, Worasak and Zhang, Jingtian and Aslam, Nauman and Shum, Hubert P. H.\n",
      "year:  2018\n",
      "type:  JOUR\n",
      "journal/booktitle:  IEEE Transactions on Neural Systems and Rehabilitation Engineering\n",
      "volume:  26\n",
      "number of pages:  10\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/TNSRE.2018.2880871\n",
      "ISSN:  15344320\n",
      "citation:  18\n",
      "Impact Factor:  3.802\n",
      "——————————————————————————————————————\n",
      "title:  A Dual-Stream Recurrent Neural Network for Student Feedback Prediction using Kinect\n",
      "abstract:  Convenience internet access and ubiquitous computing have opened up new avenues for learning and teaching. They are now no longer confined to the classroom walls, but are available to anyone connected to the internet. E-learning has opened massive opportunities for learners who otherwise would have been constrained due to geographical distances, time and/or cost factors. It has revolutionized the learning methods and represents a paradigm shift from traditional learning methods. However, despite all its advantages, e-learning is not without its own shortcomings. Understanding the effectiveness of a teaching strategy through learner feedback has been a key performance measure and decision making criteria to fine tune the teaching strategy. However, traditional methods of collecting learner feedback are inadequate in a geographically distributed, virtual setup of the e-learning environment. Innovative and novel learner feedback collection mechanism is hence the need of the hour. In this work, we design and develop a deep learning based student feedback prediction system by recognizing the subtle facial motions during a student’s learning activity. This allows the system to infer the needs of the learners as if it is a realhuman teacher in order to provide the appropriate feedback. We propose a recurrent convolutional neural network structure to understand the color and depth streams of video taken by an RGB-D camera. Experimental results have shown that our system achieve high accuracy in estimating the feedback labels. While we demonstrate the proposed framework in an e-learning setup, it can be adapted to other applications such as in-house patient monitoring and rehabilitation training. \n",
      "author:  Hu, Shanfeng and Bhattacharya, Hindol and Chattopadhyay, Matangini and Aslam, Nauman and Shum, Hubert P. H.\n",
      "year:  2018\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2018 International Conference on Software Knowledge Information Management and Applications\n",
      "volume:   \n",
      "number of pages:  8\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/SKIMA.2018.8631537\n",
      "ISSN:  25733214\n",
      "citation:  2\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Identifying Abnormal Gait in Older People during Multiple-Tasks Assessment with Audio-Visual Cues\n",
      "abstract:  This research presents a feasibility to adopt a decision support system framework as a rehabilitation and assessment tool for supporting the physiotherapist in identifying the abnormal gaits of older people. The walking movement was captured by the Microsoft Kinect cameras in order to collect the human motion during 4-meters clinical walk test. 28 older adults participated in this research and perform their gait in front of the affordable cameras. To distinguish an abnormal gait with balance impairment from those of healthy older adults, two machine learning algorithms; ANN and SVM, were selected to classify the data. Experimental results show that SVM achieves the best performance  of classification with 82.14% of accuracy, in single-task and double-task conditions, when compared with the standa rd clinical results. However, SVM cannot achieve an acceptable performance when classifying triple-task condition, achieving only 71.42% of accuracy. As a comparison, ANN delivers only 75.00% of  accuracy, which is inferior to SVM. This study show that SVM can be considered as a rehabilitation measuring tool for assisting the physiotherapist in assessing the gait of older people.\n",
      "author:  Rueangsirarak, Worasak and Kaewkaen, Kitchana and Shum, Hubert P. H.\n",
      "year:  2018\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2018 International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology\n",
      "volume:   \n",
      "number of pages:  4\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/ECTICon.2018.8619931\n",
      "ISSN:   \n",
      "citation:  1\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Validation of an Ergonomic Assessment Method using Kinect Data in Real Workplace Conditions\n",
      "abstract:  Evaluating potential musculoskeletal disorders risks in real workstations is challenging as the environment is cluttered, which makes it difficult to accurately assess workers' postures. Being marker-free and calibration-free, Microsoft Kinect is a promising device although it may be sensitive to occlusions. We propose and evaluate a RULA ergonomic assessment in real work conditions using recently published occlusion-resistant Kinect skeleton data correction. First, we compared postures estimated with this method to ground-truth data, in standardized laboratory conditions. Second, we compared RULA scores to those provided by two professional experts, in a non-laboratory cluttered workplace condition. The results show that the corrected Kinect data can provide more accurate RULA grand scores, even under sub-optimal conditions induced by the workplace environment. This study opens new perspectives in musculoskeletal risk assessment as it provides the ergonomists with 30 Hz continuous information that could be analyzed offline and in a real-time framework.\n",
      "author:  Plantard, Pierre and Shum, Hubert P. H. and Pierres, Anne-Sophie Le and Multon, Franck\n",
      "year:  2017\n",
      "type:  JOUR\n",
      "journal/booktitle:  Applied Ergonomics\n",
      "volume:  65\n",
      "number of pages:  8\n",
      "publisher:  Elsevier\n",
      "DOI:  10.1016/j.apergo.2016.10.015\n",
      "ISSN:  00036870\n",
      "citation:  140\n",
      "Impact Factor:  3.661\n",
      "——————————————————————————————————————\n",
      "title:  Posture-based and Action-based Graphs for Boxing Skill Visualization\n",
      "abstract:  Automatic evaluation of sports skills has been an active research area. However, most of the existing research focuses on low-level features such as movement speed and strength. In this work, we propose a framework for automatic motion analysis and visualization, which allows us to evaluate high-level skills such as the richness of actions, the flexibility of transitions and the unpredictability of action patterns. The core of our framework is the construction and visualization of the posture-based graph that focuses on the standard postures for launching and ending actions, as well as the action-based graph that focuses on the preference of actions and their transition probability. We further propose two numerical indices, the Connectivity Index and the Action Strategy Index, to assess skill level according to the graph. We demonstrate our framework with motions captured from different boxers. Experimental results demonstrate that our system can effectively visualize the strengths and weaknesses of the boxers.\n",
      "author:  Shen, Yijun and Wang, He and Ho, Edmond S. L. and Yang, Longzhi and Shum, Hubert P. H.\n",
      "year:  2017\n",
      "type:  JOUR\n",
      "journal/booktitle:  Computers and Graphics\n",
      "volume:  69\n",
      "number of pages:  13\n",
      "publisher:  Elsevier\n",
      "DOI:  10.1016/j.cag.2017.09.007\n",
      "ISSN:  00978493\n",
      "citation:  6\n",
      "Impact Factor:  1.936\n",
      "——————————————————————————————————————\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:  Inverse Dynamics Based on Occlusion-resistant Kinect Data: Is It Usable for Ergonomics?\n",
      "abstract:  Joint torques and forces are relevant quantities to estimate the biomechanical constraints of working tasks in ergonomics. However, inverse dynamics requires accurate motion capture data, which are generally not available in real manufacturing plants. Markerless and calibrationless measurement systems based on depth cameras, such as the Microsoft Kinect, are promising means to measure 3D poses in real time. Recent works have proposed methods to obtain reliable continuous skeleton data in cluttered environments, with occlusions and inappropriate sensor placement. In this paper, we evaluate the reliability of an inverse dynamics method based on this corrected skeleton data and its potential use to estimate joint torques and forces in such cluttered environments. To this end, we compared the calculated joint torques with those obtained with a reference inverse dynamics method based on an optoelectronic motion capture system. Results show that the Kinect skeleton data enabled the inverse dynamics process to deliver reliable joint torques in occlusion-free (r = 0.99 for the left shoulder elevation) and occluded (r = 0.91 for the left shoulder elevation) environments. However, differences remain between joint torques estimations. Such reliable joint torques open appealing perspectives for the use of new fatigue or solicitation indexes based on internal efforts measured on site.\n",
      "author:  Plantard, Pierre and Muller, Antoine and Pontonnier, Charles and Dumont, Georges and Shum, Hubert P. H. and Multon, Franck\n",
      "year:  2017\n",
      "type:  JOUR\n",
      "journal/booktitle:  International Journal of Industrial Ergonomics\n",
      "volume:  61\n",
      "number of pages:  10\n",
      "publisher:  Elsevier\n",
      "DOI:  10.1016/j.ergon.2017.05.010\n",
      "ISSN:  01698141\n",
      "citation:  31\n",
      "Impact Factor:  2.656\n",
      "——————————————————————————————————————\n",
      "title:  Usability of Corrected Kinect Measurement for Ergonomic Evaluation in Constrained Environment\n",
      "abstract:  Evaluation of potential risks of musculoskeletal disorders in real workstations is challenging as the environment is cluttered, which makes it difficult to correctly assess the pose of a worker. Being marker-free and calibration-free, Microsoft Kinect is a promising device to assess these poses, but it can deliver unreliable poses especially when occlusions occur. To overcome this problem, we propose to detect and correct badly recognized body parts thanks to a database of example poses. We applied the proposed method to compute Rapid Upper Limb Assessment (RULA) score in a realistic environment that involved sub-optimal Kinect placement and several types of occlusions. Results showed that when occlusions occur, the inaccurate raw Kinect data could be significantly improved using our correction method, leading to acceptable joint angles and RULA scores. Our method opens new perspectives to define new fatigue or solicitation indexes based on continuous measurement contrary to classical static images used in ergonomics.\n",
      "author:  Plantard, Pierre and Shum, Hubert P. H. and Multon, Franck\n",
      "year:  2017\n",
      "type:  JOUR\n",
      "journal/booktitle:  International Journal Human Factors Modelling and Simulation\n",
      "volume:  5\n",
      "number of pages:  16\n",
      "publisher:  Inderscience\n",
      "DOI:  10.1504/IJHFMS.2017.10007534\n",
      "ISSN:  17425549\n",
      "citation:  8\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  SkillVis: A Visualization Tool for Boxing Skill Assessment\n",
      "abstract:  Motion analysis and visualization are crucial in sports science for sports training and performance evaluation. While primitive computational methods have been proposed for simple analysis such as postures and movements, few can evaluate the high-level quality of sports players such as their skill levels and strategies. We propose a visualization tool to help visualizing boxers' motions and assess their skill levels. Our system automatically builds a graph-based representation from motion capture data and reduces the dimension of the graph onto a 3D space so that it can be easily visualized and understood. In particular, our system allows easy understanding of the boxer's boxing behaviours, preferred actions, potential strength and weakness. We demonstrate the effectiveness of our system on different boxers' motions. Our system not only serves as a tool for visualization, it also provides intuitive motion analysis that can be further used beyond sports science.\n",
      "author:  Shum, Hubert P. H. and Wang, He and Ho, Edmond S. L. and Komura, Taku\n",
      "year:  2016\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2016 International Conference on Motion in Games\n",
      "volume:   \n",
      "number of pages:  9\n",
      "publisher:  ACM\n",
      "DOI:  10.1145/2994258.2994266\n",
      "ISSN:  9781450345927\n",
      "citation:  3\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Ergonomics Measurements using Kinect with a Pose Correction Framework\n",
      "abstract:  Evaluation of potential risks of musculoskeletal disorders in real workstations is challenging as the environment is cluttered, which makes it difficult to correctly and accurately assess the pose of a worker. Being marker-free and calibration-free, Microsoft Kinect is a promising device to assess these poses, but it can deliver unreliable poses especially when occlusions occur. To overcome this problem, we propose to detect badly recognized body parts and to replace them by an appropriate combination of example poses gathered in a pre-recorded pose. The main contribution of this work is to organize the database as a filtered pose graph structure that enables the system to select relevant candidates for the combination: candidates that ensure continuity with the previous pose and similarity with the available reliable information. We applied the proposed method in a realistic environment that involved sub-optimal Kinect placement and several types of occlusions. An optoelectronic motion capture system was concurrently used to obtain ground truth joint angles. In an ergonomics context, we also computed Rapid Upper Limb Assessment RULA scores. This kind of ergonomics tool requires to rate the pose of the worker based on an estimation of the joint angles. These latter are then used to provide a global risk score. Results showed that when occlusions occur, the inaccurate raw Kinect data could be significantly improved using our correction method, leading to acceptable joint angles. As RULA calculation is based on angular thresholds, which tends to minimize the effect of joint angle errors, when these error values are not close to thresholds. However, for realistic scenarios with occlusions that lead to very large joint angle errors, the correction method also provided significantly better RULA scores. Our method opens new perspectives to define new fatigue or solicitation indexes based on continuous measurement contrary to classical static images used in ergonomics. As the computation time is very low, it also enables real-time feedback and interaction with the operator.\n",
      "author:  Plantard, Pierre and Shum, Hubert P. H. and Multon, Franck\n",
      "year:  2016\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2016 International Digital Human Modeling Symposium\n",
      "volume:   \n",
      "number of pages:  8\n",
      "publisher:   \n",
      "DOI:   \n",
      "ISSN:   \n",
      "citation:  6\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Automatic Evaluation of Boxing Techniques from Captured Shadow Boxing Data\n",
      "abstract:  We propose a fully automatic procedure to preprocess captured shadow boxing motion and evaluate the boxer's techniques. The captured shadow boxing motions are automatically segmented and classified into different actions such as straight punches, hooks, or translations. Then, it is possible to automatically evaluate their skills by checking the quality of their movements and finding out which kind of motions are available by the boxers. It is also possible to compose a Motion Graph of the fighters and simulate the matches of different boxers to find out their weak points. The proposed method is useful for boxers to periodically check their performances.\n",
      "author:  Shum, Hubert P. H. and Komura, Taku and Nagano, Akinori\n",
      "year:  2007\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2007 Congress of International Society of Biomechanics\n",
      "volume:  40\n",
      "number of pages:  1\n",
      "publisher:   \n",
      "DOI:  10.1016/S00219290(07)702737\n",
      "ISSN:   \n",
      "citation:  0\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:  A Two-Stream Recurrent Network for Skeleton-Based Human Interaction Recognition\n",
      "abstract:  This paper addresses the problem of recognizing human-human interaction from skeletal sequences. Existing works are mainly designed to classify single human action. Many of them simply stack the movement features of two characters to deal with human interaction, while neglecting the abundant relationships between characters. In this paper, we propose a novel two-stream recurrent neural network by adopting the geometric features from both single actions and interactions to describe the spatial correlations with different discriminative abilities. The first stream is constructed under pairwise joint distance (PJD) in a fully-connected mesh to categorize the interactions with explicit distance patterns. To better distinguish similar interactions, in the second stream, we combine PJD with the spatial features from individual joint positions using graph convolutions to detect the implicit correlations among joints, where the joint connections in graph is adaptive for flexible correlations. After spatial modeling, each stream is fed to a bidirectional LSTM to encode two-way temporal properties. To take advantage of the diverse discriminative power of the two streams, we come up with a late fusion algorithm to combine their output predictions concerning information entropy. Experimental results show that the proposed framework achieves state-of-the-art performance on 3D and comparable performance on 2D interaction datasets. Moreover, the late fusion results demonstrate the effectiveness of improving the recognition accuracy compared with single streams.\n",
      "author:  Men, Qianhui and Leung, Howard and Ho, Edmond S. L. and Shum, Hubert P. H.\n",
      "year:  2020\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2020 International Conference on Pattern Recognition\n",
      "volume:   \n",
      "number of pages:  8\n",
      "publisher:   \n",
      "DOI:  10.1109/ICPR48806.2021.9412538\n",
      "ISSN:   \n",
      "citation:  4\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Serious Games with Human-Object Interactions using RGB-D Camera\n",
      "abstract:  Commercial RGB-D cameras typically require a clear posture without occlusion. This hugely limits the usability of the device for serious applications that require manipulation of external objects. In this paper, we propose an integrated framework to track motion and object during human-object interactions. We implement a data-driven posture reconstruction algorithm to correct wrongly tracked body parts during occlusions, as well as a computer vision based object tracking algorithm using the depth image. We demonstrate preliminary results in which the system tracks a user playing with a basketball.\n",
      "author:  Shum, Hubert P. H.\n",
      "year:  2013\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 6th International Conference on Motion in Games\n",
      "volume:   \n",
      "number of pages:  1\n",
      "publisher:  SpringerVerlag\n",
      "DOI:   \n",
      "ISSN:   \n",
      "citation:  1\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  3D Car Shape Reconstruction from a Contour Sketch using GAN and Lazy Learning\n",
      "abstract:  3D car models are heavily used in computer games, visual effects, and even automotive designs. As a result, producing such models with minimal labour costs is increasingly more important. To tackle the challenge, we propose a novel system to reconstruct a 3D car using a single sketch image. The system learns from a synthetic database of 3D car models and their corresponding 2D contour sketches and segmentation masks, allowing effective training with minimal data collection cost. The core of the system is a machine learning pipeline that combines the use of a Generative Adversarial Network (GAN) and lazy learning. GAN, being a deep learning method, is capable of modelling complicated data distributions, enabling the effective modelling of a large variety of cars. Its major weakness is that as a global method, modelling the fine details in the local region is challenging. Lazy learning works well to preserve local features by generating a local subspace with relevant data samples. We demonstrate that the combined use of GAN and lazy learning produces is able to produce high-quality results, in which different types of cars with complicated local features can be generated effectively with a single sketch. Our method outperforms existing ones using other machine learning structures such as the variational autoencoder.\n",
      "author:  Nozawa, Naoki and Shum, Hubert P. H. and Ho, Edmond S. L. and Feng, Qi\n",
      "year:  2021\n",
      "type:  JOUR\n",
      "journal/booktitle:  Visual Computer\n",
      "volume:   \n",
      "number of pages:  14\n",
      "publisher:  Springer\n",
      "DOI:  10.1007/s0037102002024y\n",
      "ISSN:  14322315\n",
      "citation:  1\n",
      "Impact Factor:  2.601\n",
      "——————————————————————————————————————\n",
      "title:  DurLAR: A High-fidelity 128-channel LiDAR Dataset with Panoramic Ambientand Reflectivity Imagery for Multi-modal Autonomous Driving Applications\n",
      "abstract:  We present DurLAR, a high-fidelity 128-channel 3D Li- DAR dataset with panoramic ambient (near infrared) and reflectivity imagery, as well as a sample benchmark task using depth estimation for autonomous driving applications. Our driving platform is equipped with a high resolution 128 channel LiDAR, a 2MPix stereo camera, a lux meter and a GNSS/INS system. Ambient and reflectivity images are made available along with the LiDAR point clouds to facilitate multi-modal use of concurrent ambient and reflectivity scene information. Leveraging DurLAR, with a resolution exceeding that of prior benchmarks, we consider the task of monocular depth estimation and use this increased availability of higher resolution, yet sparse ground truth scene depth information to propose a novel joint supervised/selfsupervised loss formulation. We compare performance over both our new DurLAR dataset, the established KITTI benchmark and the Cityscapes dataset. Our evaluation shows our joint use supervised and self-supervised loss terms, enabled via the superior ground truth resolution and availability within DurLAR improves the quantitative and qualitative performance of leading contemporary monocular depth estimation approaches (RMSE = 3.639, SqRel = 0.936). \n",
      "author:  Li, Li and Ismail, Khalid N. and Shum, Hubert P. H. and Breckon, Toby P.\n",
      "year:  2021\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2021 International Conference on 3D Vision\n",
      "volume:   \n",
      "number of pages:   \n",
      "publisher:   \n",
      "DOI:   \n",
      "ISSN:   \n",
      "citation:  2\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Bi-projection based Foreground-aware Omnidirectional Depth Prediction\n",
      "abstract:  Due to the increasing availability of commercial 360-degree cameras, accurate depth prediction for omnidirectional images can be beneficial to a wide range of applications including video editing and augmented reality. Regarding existing methods, some focus on learning high-quality global prediction while fail to capture detailed local features. Others suggest integrating local context into the learning procedure, they yet propose to train on non-foreground-aware databases. In this paper, we explore to simultaneously use equirectangular and cubemap projection to learn omnidirectional depth prediction from foreground-aware databases in a multi-task manner. Experimental results demonstrate improved performance when compared to the state-of-the-art.\n",
      "author:  Feng, Qi and Shum, Hubert P. H. and Morishima, Shigeo\n",
      "year:  2021\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2021 Visual Computing\n",
      "volume:   \n",
      "number of pages:  6\n",
      "publisher:   \n",
      "DOI:   \n",
      "ISSN:   \n",
      "citation:  0\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Foreground-aware Dense Depth Estimation for 360 Images\n",
      "abstract:  With 360 imaging devices becoming widely accessible, omnidirectional content has gained popularity in multiple fields. The ability to estimate depth from a single omnidirectional image can benefit applications such as robotics navigation and virtual reality. However, existing depth estimation approaches produce sub-optimal results on real-world omnidirectional images with dynamic foreground objects. On the one hand, capture-based methods cannot obtain the foreground due to the limitations of the scanning and stitching schemes. On the other hand, it is challenging for synthesis-based methods to generate highly-realistic virtual foreground objects that are comparable to the real-world ones. In this paper, we propose to augment datasets with realistic foreground objects using an image-based approach, which produces a foreground-aware photorealistic dataset for machine learning algorithms. By exploiting a novel scale-invariant RGB-D correspondence in the spherical domain, we repurpose abundant non-omnidirectional datasets to include realistic foreground objects with correct distortions. We further propose a novel auxiliary deep neural network to estimate both the depth of the omnidirectional images and the mask of the foreground objects, where the two tasks facilitate each other. A new local depth loss considers small regions of interests and ensures that their depth estimations are not smoothed out during the global gradient’s optimization. We demonstrate the system using human as the foreground due to its complexity and contextual importance, while the framework can be generalized to any other foreground objects. Experimental results demonstrate more consistent global estimations and more accurate local estimations compared with state-of-the-arts. \n",
      "author:  Feng, Qi and Shum, Hubert P. H. and Morishima, Shigeo\n",
      "year:  2020\n",
      "type:  JOUR\n",
      "journal/booktitle:  Journal of WSCG\n",
      "volume:  28\n",
      "number of pages:  10\n",
      "publisher:   \n",
      "DOI:  10.24132/JWSCG.2020.28.10\n",
      "ISSN:  12136972\n",
      "citation:  1\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Single Sketch Image based 3D Car Shape Reconstruction with Deep Learning and Lazy Learning\n",
      "abstract:  Efficient car shape design is a challenging problem in both the automotive industry and the computer animation/games industry. In this paper, we present a system to reconstruct the 3D car shape from a single 2D sketch image. To learn the correlation between 2D sketches and 3D cars, we propose a Variational Autoencoder deep neural network that takes a 2D sketch and generates a set of multi-view depth and mask images, which form a more effective representation comparing to 3D meshes, and can be effectively fused to generate a 3D car shape. Since global models like deep learning have limited capacity to reconstruct fine-detail features, we propose a local lazy learning approach that constructs a small subspace based on a few relevant car samples in the database. Due to the small size of such a subspace, fine details can be represented effectively with a small number of parameters. With a low-cost optimization process, a high-quality car shape with detailed features is created. Experimental results show that the system performs consistently to create highly realistic cars of substantially different shape and topology.\n",
      "author:  Nozawa, Naoki and Shum, Hubert P. H. and Ho, Edmond S. L. and Morishima, Shigeo\n",
      "year:  2020\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2020 International Conference on Computer Graphics Theory and Applications\n",
      "volume:   \n",
      "number of pages:  12\n",
      "publisher:  SciTePress\n",
      "DOI:  10.5220/0009157001790190\n",
      "ISSN:  21844321\n",
      "citation:  1\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  3D Car Shape Reconstruction from a Single Sketch Image\n",
      "abstract:  Efficient car shape design is a challenging problem in both the automotive industry and the computer animation/games industry. In this paper, we present a system to reconstruct the 3D car shape from a single 2D sketch image. To learn the correlation between 2D sketches and 3D cars, we propose a Variational Autoencoder deep neural network that takes a 2D sketch and generates a set of multiview depth & mask images, which are more effective representation comparing to 3D mesh, and can be combined to form the 3D car shape. To ensure the volume and diversity of the training data, we propose a feature-preserving car mesh augmentation pipeline for data augmentation. Since deep learning has limited capacity to reconstruct fine-detail features, we propose a lazy learning approach that constructs a small subspace based on a few relevant car samples in the database. Due to the small size of such a subspace, fine details can be represented effectively with a small number of parameters. With a low-cost optimization process, a high-quality car with detailed features is created. Experimental results show that the system performs consistently to create highly realistic cars of substantially different shape and topology, with a very low computational cost.\n",
      "author:  Nozawa, Naoki and Shum, Hubert P. H. and Ho, Edmond S. L. and Morishima, Shigeo\n",
      "year:  2019\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2019 International Conference on Motion in Games\n",
      "volume:   \n",
      "number of pages:  2\n",
      "publisher:  ACM\n",
      "DOI:  10.1145/3359566.3364693\n",
      "ISSN:  9781450369947\n",
      "citation:  0\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Prior-less 3D Human Shape Reconstruction with an Earth Mover's Distance Informed CNN\n",
      "abstract:  We propose a novel end-to-end deep learning framework, capable of 3D human shape reconstruction from a 2D image without the need of a 3D prior parametric model. We employ a \"prior-less\" representation of the human shape using unordered point clouds. Due to the lack of prior information, comparing the generated and ground truth point clouds to evaluate the reconstruction error is challenging. We solve this problem by proposing an Earth Mover’s Distance (EMD) function to find the optimal mapping between point clouds. Our experimental results show that we are able to obtain a visually accurate estimation of the 3D human shape from a single 2D image, with some inaccuracy for heavily occluded parts. \n",
      "author:  Zhang, Jingtian and Shum, Hubert P. H. and McCay, Kevin D. and Ho, Edmond S. L.\n",
      "year:  2019\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2019 International Conference on Motion in Games\n",
      "volume:   \n",
      "number of pages:  2\n",
      "publisher:  ACM\n",
      "DOI:  10.1145/3359566.3364694\n",
      "ISSN:  9781450369947\n",
      "citation:  1\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Filtered Pose Graph for Efficient Kinect Pose Reconstruction\n",
      "abstract:  Being marker-free and calibration free, Microsoft Kinect is nowadays widely used in many motion-based applications, such as user training for complex industrial tasks and ergonomics pose evaluation. The major problem of Kinect is the placement requirement to obtain accurate poses, as well as its weakness against occlusions. To improve the robustness of Kinect in interactive motion-based applications, real-time data-driven pose reconstruction has been proposed. The idea is to utilize a database of accurately captured human poses as a prior to optimize the Kinect recognized ones, in order to estimate the true poses performed by the user. The key research problem is to identify the most relevant poses in the database for accurate and efficient reconstruction. In this paper, we propose a new pose reconstruction method based on modelling the pose database with a structure called Filtered Pose Graph, which indicates the intrinsic correspondence between poses. Such a graph not only speeds up the database poses selection process, but also improves the relevance of the selected poses for higher quality reconstruction. We apply the proposed method in a challenging environment of industrial context that involves sub-optimal Kinect placement and a large amount of occlusion. Experimental results show that our real-time system reconstructs Kinect poses more accurately than existing methods. \n",
      "author:  Plantard, Pierre and Shum, Hubert P. H. and Multon, Franck\n",
      "year:  2017\n",
      "type:  JOUR\n",
      "journal/booktitle:  Multimedia Tools and Applications\n",
      "volume:  76\n",
      "number of pages:  22\n",
      "publisher:  SpringerVerlag\n",
      "DOI:  10.1007/s1104201635464\n",
      "ISSN:  15737721\n",
      "citation:  45\n",
      "Impact Factor:  2.757\n",
      "——————————————————————————————————————\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:  Kinect Posture Reconstruction based on a Local Mixture of Gaussian Process Models\n",
      "abstract:  Depth sensor based 3D human motion estimation hardware such as Kinect has made interactive applications more popular recently. However, it is still challenging to accurately recognize postures from a single depth camera due to the inherently noisy data derived from depth images and self-occluding action performed by the user. In this paper, we propose a new real-time probabilistic framework to enhance the accuracy of live captured postures that belong to one of the action classes in the database. We adopt the Gaussian Process model as a prior to leverage the position data obtained from Kinect and marker-based motion capture system. We also incorporate a temporal consistency term into the optimization framework to constrain the velocity variations between successive frames. To ensure that the reconstructed posture resembles the accurate parts of the observed posture, we embed a set of joint reliability measurements into the optimization framework. A major drawback of Gaussian Process is its cubic learning complexity when dealing with a large database due to the inverse of a covariance matrix. To solve the problem, we propose a new method based on a local mixture of Gaussian Processes, in which Gaussian Processes are defined in local regions of the state space. Due to the significantly decreased sample size in each local Gaussian Process, the learning time is greatly reduced. At the same time, the prediction speed is enhanced as the weighted mean prediction for a given sample is determined by the nearby local models only. Our system also allows incrementally updating a specific local Gaussian Process in real time, which enhances the likelihood of adapting to run-time postures that are different from those in the database. Experimental results demonstrate that our system can generate high quality postures even under severe self-occlusion situations, which is beneficial for real-time applications such as motion-based gaming and sport training.\n",
      "author:  Liu, Zhiguang and Zhou, Liuyang and Leung, Howard and Shum, Hubert P. H.\n",
      "year:  2016\n",
      "type:  JOUR\n",
      "journal/booktitle:  IEEE Transactions on Visualization and Computer Graphics\n",
      "volume:  22\n",
      "number of pages:  14\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/TVCG.2015.2510000\n",
      "ISSN:  10772626\n",
      "citation:  67\n",
      "Impact Factor:  4.579\n",
      "——————————————————————————————————————\n",
      "title:  Posture Reconstruction Using Kinect with a Probabilistic Model\n",
      "abstract:  Recent work has shown that depth image based 3D posture estimation hardware such as Kinect has made interactive applications more popular. However, it is still challenging to accurately recognize postures from a single depth camera due to the inherently noisy data derived from depth images and self-occluding action performed by the user. While previous research has shown that data-driven methods can be used to reconstruct the correct postures, they usually require a large posture database, which greatly limit the usability for systems with constrained hardware such as game console. To solve this problem, we present a new probabilistic framework to enhance the accuracy of the postures live captured by Kinect. We adopt the Gaussian Process model as a prior to leverage position data obtained from Kinect and marker-based motion capture system. We also incorporate a temporal consistency term into the optimization framework to constrain the velocity variations between successive frames. To ensure that the reconstructed posture resembles the observed input data from Kinect when its tracking result is good, we embed joint reliability into the optimization framework. Experimental results demonstrate that our system can generate high quality postures even under severe self-occlusion situations, which is beneficial for real-time posture based applications such as motion-based gaming and sport training. \n",
      "author:  Zhou, Liuyang and Liu, Zhiguang and Leung, Howard and Shum, Hubert P. H.\n",
      "year:  2014\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 20th ACM Symposium on Virtual Reality Software and Technology\n",
      "volume:   \n",
      "number of pages:  9\n",
      "publisher:  ACM\n",
      "DOI:  10.1145/2671015.2671021\n",
      "ISSN:   \n",
      "citation:  44\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Real-Time Posture Reconstruction for Microsoft Kinect\n",
      "abstract:  The recent advancement of motion recognition using Microsoft Kinect stimulates many new ideas in motion capture and virtual reality applications. Utilizing a pattern recognition algorithm, Kinect can determine the positions of different body parts from the user. However, due to the use of a single depth camera, recognition accuracy drops significantly when the parts are occluded. This hugely limits the usability of applications that involves interaction with external objects, such as sport training or exercising systems. The problem becomes more critical when Kinect incorrectly perceives the body parts. This is because applications have limited information about the recognition correctness, and using those parts to synthesize a body postures would result in serious visual artifacts. In this paper, we propose a new method to reconstruct valid movement from incomplete and noisy postures captured by Kinect. We first design a set of measurements that objectively evaluates the degree of reliability on each tracked body part. By incorporating the reliability estimation into a motion database query during run-time, we obtain a set of similar postures that are kinematically valid. These postures are used to construct a latent space, which is known as the natural posture space in our system, with local Principle Component Analysis (PCA). We finally apply frame-based optimization in the space to synthesize a new posture that closely resembles the true user posture while satisfying kinematic constraints. Experimental results show that our method can significantly improve the quality of the recognized posture under severely occluded environments, such as a person exercising with a basketball or moving in a small room.\n",
      "author:  Shum, Hubert P. H. and Ho, Edmond S. L. and Jiang, Yang and Takagi, Shu\n",
      "year:  2013\n",
      "type:  JOUR\n",
      "journal/booktitle:  IEEE Transactions on Cybernetics\n",
      "volume:  43\n",
      "number of pages:  13\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/TCYB.2013.2275945\n",
      "ISSN:  21682267\n",
      "citation:  212\n",
      "Impact Factor:  11.448\n",
      "——————————————————————————————————————\n",
      "title:  Environment Capturing with Microsoft Kinect\n",
      "abstract:  Constructing virtual scenes that incorporate human and object interaction has traditionally been a time consuming process in computer animation, whereby the motion of an actor is first recorded and any objects used in the scene are then intricately added by an animator. The Microsoft Kinect utilizes a synchronized RGBD stream to provide markerless skeletal tracking of humans, enabling efficient motion capture; however, the problem of capturing environment objects remains unsolved. In this paper, we propose a new framework to segment and track three major types of environment objects using Kinect, namely background planes, stationary objects and dynamic objects. We demonstrate that the motion of an actor and their surrounding environment can be obtained at the same time, saving considerable effort for the animators. Our proposed system is best to be applied to applications involving extensive human-object interactions, such as console games and animation designs.\n",
      "author:  Mackay, Kevin and Shum, Hubert P. H. and Komura, Taku\n",
      "year:  2012\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2012 International Conference on Software Knowledge Information Management and Applications\n",
      "volume:   \n",
      "number of pages:  6\n",
      "publisher:   \n",
      "DOI:   \n",
      "ISSN:   \n",
      "citation:  2\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Action Recognition from Arbitrary Views Using Transferable Dictionary Learning\n",
      "abstract:  Human action recognition is crucial to many practical applications, ranging from human-computer interaction to video surveillance. Most approaches either recognize the human action from a fixed view or require the knowledge of view angle, which is usually not available in practical applications. In this paper, we propose a novel end-to-end framework to jointly learn a view-invariance transfer dictionary and a view-invariant classifier. The result of the process is a dictionary that can project real-world 2D video into a view-invariant sparse representation, and a classifier to recognize actions with an arbitrary view. The main feature of our algorithm is the use of synthetic data to extract view-invariance between 3D and 2D videos during the pre-training phase. This guarantees the availability of training data, and removes the hassle of obtaining real-world videos in specific viewing angles. Additionally, for better describing the actions in 3D videos, we introduce a new feature set called the3D dense trajectories to effectively encode extracted trajectory information on 3D videos. Experimental results on the IXMAS, N-UCLA, i3DPost and UWA3DII data sets show improvements over existing algorithms.\n",
      "author:  Zhang, Jingtian and Shum, Hubert P. H. and Han, Jungong and Shao, Ling\n",
      "year:  2018\n",
      "type:  JOUR\n",
      "journal/booktitle:  IEEE Transactions on Image Processing\n",
      "volume:  27\n",
      "number of pages:  15\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/TIP.2018.2836323\n",
      "ISSN:  10577149\n",
      "citation:  32\n",
      "Impact Factor:  10.856\n",
      "——————————————————————————————————————\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:  CCESK: A Chinese Character Educational System Based on Kinect\n",
      "abstract:  In this paper, a Chinese character educational system based on Kinect is proposed for guiding beginners to learn the basics of Chinese characters in a more intuitive way. It extracts 19 common components, denoted as alphabets, from Chinese characters. 19 postures were designed according to the shapes of these alphabets. Instead of memorizing Chinese characters through repetitive copying, students can first associate an alphabet with the corresponding designed posture. Then they break down a Chinese character into a set of alphabets in order to perform the sequence of corresponding postures so that they can easily remember the whole character. Our proposed system contains 2 major functions: 1) the learning function that is responsible for delivering the courseware, and 2) the testing function that is used to let students acquire their learning progress through some tests. A rule-based algorithm is designed to recognize the students' input postures captured from the Kinect motion sensor so as to determine whether the students have performed the correct postures. We conducted a survey which involved 90 students to try our proposed system as well as two other learning modes for comparison. Moreover, we have interviewed those 30 students who had tried our proposed system with some open-ended questions. The positive results show that the proposed system can promote students' experiences in learning Chinese characters.\n",
      "author:  Yang, Yang and Leung, Howard and Shum, Hubert P. H. and Li, Jiao and Zeng, Lanling and Aslam, Nauman and Pan, Zhigeng\n",
      "year:  2018\n",
      "type:  JOUR\n",
      "journal/booktitle:  IEEE Transactions on Learning Technologies\n",
      "volume:  11\n",
      "number of pages:  6\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/TLT.2017.2723888\n",
      "ISSN:  19391382\n",
      "citation:  14\n",
      "Impact Factor:  3.720\n",
      "——————————————————————————————————————\n",
      "title:  Saliency-Informed Spatio-Temporal Vector of Locally Aggregated Descriptors and Fisher Vectors for Visual Action Recognition\n",
      "abstract:  Feature encoding has been extensively studied for the task of visual action recognition (VAR). The recently proposed super vector-based encoding methods, such as the Vector of Locally Aggregated Descriptors (VLAD) and the Fisher Vectors (FV), have significantly improved the recognition performance. Despite of the success, they still struggle with the superfluous information that presents during the training stage, which makes the methods computationally expensive when applied to a large number of extracted features. In order to address such challenge, this paper proposes a Saliency-Informed Spatio-Temporal VLAD (SST-VLAD) approach which selects the extracted features corresponding to small amount of videos in the data set by considering both the spatial and temporal video-wise saliency scores; and the same extension principle has also been applied to the FV approach. The experimental results indicate that the proposed feature encoding schemes consistently outperform the existing ones with significantly lower computational cost.\n",
      "author:  Zuo, Zheming and Organisciak, Daniel and Shum, Hubert P. H. and Yang, Longzhi\n",
      "year:  2018\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2018 British Machine Vision Conference Workshop on Image Analysis for Human Facial and Activity Recognition\n",
      "volume:   \n",
      "number of pages:  11\n",
      "publisher:   \n",
      "DOI:   \n",
      "ISSN:   \n",
      "citation:  4\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  A Motion Classification Approach to Fall Detection\n",
      "abstract:  The population of older people in the world has grown rapidly in recent years. To alleviate the increasing burden on health systems, automated health monitoring of older people can be very economical for requesting urgent medical support when a harmful accident has been detected. One of the accidents that happens frequently to older people in a household environment is a fall, which can cause serious injuries if not handled immediately. In this paper, we propose a motion classification approach to fall detection, by integrating the techniques of motion capture and machine learning. The motion of a person is recorded with a set of inertial sensors, which provides a comprehensive and structural description of body movements, while being robust to variations in the working environment. We build a database comprising motions of both falls and normal activities. We experiment with several combinations of joint selection, feature extraction, and classification algorithms, showing that accurate fall detection can be achieved by our motion classification approach.\n",
      "author:  Hu, Shanfeng and Rueangsirarak, Worasak and Bouchee, Maxime and Aslam, Nauman and Shum, Hubert P. H.\n",
      "year:  2017\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2017 International Conference on Software Knowledge Information Management and Applications\n",
      "volume:   \n",
      "number of pages:  6\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/SKIMA.2017.8294096\n",
      "ISSN:  25733214\n",
      "citation:  3\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Improving Posture Classification Accuracy for Depth Sensor-based Human Activity Monitoring in Smart Environments\n",
      "abstract:  Smart environments and monitoring systems are popular research areas nowadays due to its potential to enhance the quality of life. Applications such as human behaviour analysis and workspace ergonomics monitoring are automated, thereby improving well-being of individuals with minimal running cost. The central problem of smart environments is to understand what the user is doing in order to provide the appropriate support. While it is difficult to obtain information of full body movement in the past, depth camera-based motion sensing technology such as Kinect has made it possible to obtain 3D posture without complex setup. This has fused a large number of research projects to apply Kinect in smart environments. The common bottleneck of these researches is the high amount of errors in the detected joint positions, which would result in inaccurate analysis and false alarms. In this paper, we propose a framework that accurately classifies the nature of the 3D postures obtained by Kinect using a max-margin classifier. Different from previous work in the area, we integrate the information about the reliability of the tracked joints in order to enhance the accuracy and robustness of our framework. As a result, apart from general classifying activity of different movement context, our proposed method can classify the subtle differences between correctly performed and incorrectly performed movement in the same context. We demonstrate how our framework can be applied to evaluate the user??s posture and identify the postures that may result in musculoskeletal disorders. Such a system can be used in workplace such as offices and factories to reduce risk of injury. Experimental results have shown that our method consistently outperforms existing algorithms in both activity classification and posture healthiness classification. Due to the low-cost and the easy deployment process of depth camera based motion sensors, our framework can be applied widely in home and office to facilitate smart environments.\n",
      "author:  Ho, Edmond S. L. and Chan, Jacky C. P. and Chan, Donald C. K. and Shum, Hubert P. H. and Cheung, Yiu-ming and Yuen, P. C.\n",
      "year:  2016\n",
      "type:  JOUR\n",
      "journal/booktitle:  Computer Vision and Image Understanding\n",
      "volume:  148\n",
      "number of pages:  14\n",
      "publisher:  Elsevier\n",
      "DOI:  10.1016/j.cviu.2015.12.011\n",
      "ISSN:  10773142\n",
      "citation:  55\n",
      "Impact Factor:  3.876\n",
      "——————————————————————————————————————\n",
      "title:  Arbitrary View Action Recognition via Transfer Dictionary Learning on Synthetic Training Data\n",
      "abstract:  Human action recognition is an important problem in robotic vision. Traditional recognition algorithms usually require the knowledge of view angle, which is not always available in robotic applications such as active vision. In this paper, we propose a new framework to recognize actions with arbitrary views. A main feature of our algorithm is that view-invariance is learned from synthetic 2D and 3D training data using transfer dictionary learning. This guarantees the availability of training data, and removes the hassle of obtaining real world video in specific viewing angles. The result of the process is a dictionary that can project real world 2D video into a view-invariant sparse representation. This facilitates the training of a view-invariant classifier. Experimental results on the IXMAS and N-UCLA datasets show significant improvements over existing algorithms.\n",
      "author:  Zhang, Jingtian and Zhang, Lining and Shum, Hubert P. H. and Shao, Ling\n",
      "year:  2016\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2016 IEEE International Conference on Robotics and Automation\n",
      "volume:   \n",
      "number of pages:  8\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/ICRA.2016.7487309\n",
      "ISSN:   \n",
      "citation:  20\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:  Human Action Recognition via Skeletal and Depth based Feature Fusion\n",
      "abstract:  This paper addresses the problem of recognizing human actions captured with depth cameras. Human action recognition is a challenging task as the articulated action data is high dimensional in both spatial and temporal domains. An effective approach to handle this complexity is to divide human body into different body parts according to human skeletal joint positions, and performs recognition based on these part-based feature descriptors. Since different types of features could share some similar hidden structures, and different actions may be well characterized by properties common to all features (sharable structure) and those specific to a feature (specific structure), we propose a joint group sparse regression-based learning method to model each action. Our method can mine the sharable and specific structures among its part-based multiple features meanwhile imposing the importance of these part-based feature structures by joint group sparse regularization, in favor of discriminative part-based feature structure selection. To represent the dynamics and appearance of the human body parts, we employ part-based multiple features extracted from skeleton and depth data respectively. Then, using the group sparse regularization techniques, we have derived an algorithm for mining the key part-based features in the proposed learning framework. The resulting features derived from the learnt weight matrices are more discriminative for multi-task classification. Through extensive experiments on three public datasets, we demonstrate that our approach outperforms existing methods.\n",
      "author:  Li, Meng and Leung, Howard and Shum, Hubert P. H.\n",
      "year:  2016\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2016 International Conference on Motion in Games\n",
      "volume:   \n",
      "number of pages:  10\n",
      "publisher:  ACM\n",
      "DOI:  10.1145/2994258.2994268\n",
      "ISSN:  9781450345927\n",
      "citation:  22\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Temporal Clustering of Motion Capture Data with Optimal Partitioning\n",
      "abstract:  Motion capture data can be characterized as a series of multi-dimensional spatio-temporal data, which is recorded by tracking the number of key points in space over time with a 3-dimensioanl representation. Such complex characteristics make the processing of motion capture data a non-trivial task. Hence, techniques that can provide an approximated, less complicated representation of such data are highly desirable. In this paper, we propose a novel technique that uses temporal clustering to generate an approximate representation of motion capture data. First, we segment the motion in the time domain with an optimal partition algorithm so that the within-segment sum of squared error (WSSSE) is minimized. Then, we represent the motion capture data as the averages taken over all the segments, resulting in a representation of much lower complexity. Experimental results suggest that comparing with the state-of-the-art methods, our proposed representation technique can better approximate the motion capture data.\n",
      "author:  Yang, Yang and Bian, Huiwen and Shum, Hubert P. H. and Aslam, Nauman and Zeng, Lanling\n",
      "year:  2016\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2016 International Conference on VirtualReality Continuum and its Applications in Industry\n",
      "volume:   \n",
      "number of pages:  4\n",
      "publisher:  ACM\n",
      "DOI:  10.1145/3013971.3014019\n",
      "ISSN:  9781450346924\n",
      "citation:  2\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Emulating Human Perception of Motion Similarity\n",
      "abstract:  Evaluating the similarity of motions is useful for motion retrieval, motion blending, and performance analysis of dancers and athletes. Euclidean distance between corresponding joints has been widely adopted in measuring similarity of postures and hence motions. However, such a measure does not necessarily conform to the human perception of motion similarity. In this paper, we propose a new similarity measure based on machine learning techniques. We make use of the results of questionnaires from subjects answering whether arbitrary pairs of motions appear similar or not. Using the relative distance between the joints as the basic features, we train the system to compute the similarity of arbitrary pair of motions. Experimental results show that our method outperforms methods based on Euclidean distance between corresponding joints. Our method is applicable to content-based motion retrieval of human motion for large-scale database systems. It is also applicable to e-Learning systems which automatically evaluate the performance of dancers and athletes by comparing the subjects' motions with those by experts.\n",
      "author:  Tang, Jeff K. T. and Leung, Howard and Komura, Taku and Shum, Hubert P. H.\n",
      "year:  2008\n",
      "type:  JOUR\n",
      "journal/booktitle:  Computer Animation and Virtual Worlds\n",
      "volume:  19\n",
      "number of pages:  11\n",
      "publisher:  John Wiley and Sons Ltd.\n",
      "DOI:  10.1002/cav.v19:3/4\n",
      "ISSN:  15464261\n",
      "citation:  63\n",
      "Impact Factor:  1.020\n",
      "——————————————————————————————————————\n",
      "title:  Finding Repetitive Patterns in 3D Human Motion Captured Data\n",
      "abstract:  Finding repetitive patterns is important to many applications such as bioinformatics, finance and speech processing, etc. Repetitive patterns can be either cyclic or acyclic such that the patterns are continuous and distributed respectively. In this paper, we are going to find repetitive patterns in a given motion signal without prior knowledge about the type of motion. It is relatively easier to find repetitive patterns in discrete signal that contains a limited number of states by dynamic programming. However, it is impractical to identify exactly matched states in a continuous signal such as captured human motion data. A point cloud similarity of the input motion signal itself is considered and the longest similar patterns are located by tracing and extending matched posture pairs. Through pattern alignment and auto-clustering, cyclic and acyclic patterns are identified. Experiment results show that our approach can locate repetitive movements with small error rates.\n",
      "author:  Tang, Jeff K. T. and Leung, Howard and Komura, Taku and Shum, Hubert P. H.\n",
      "year:  2008\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2nd international Conference on Ubiquitous Information Management and Communication\n",
      "volume:   \n",
      "number of pages:  8\n",
      "publisher:  ACM\n",
      "DOI:  10.1145/1352793.1352876\n",
      "ISSN:  9781595939937\n",
      "citation:  28\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  A Unified Deep Metric Representation for Mesh Saliency Detection and Non-rigid Shape Matching\n",
      "abstract:  In this paper, we propose a deep metric for unifying the representation of mesh saliency detection and non-rigid shape matching. While saliency detection and shape matching are two closely related and fundamental tasks in shape analysis, previous methods approach them separately and independently, failing to exploit their mutually beneficial underlying relationship. In view of the existing gap between saliency and matching, we propose to solve them together using a unified metric representation of surface meshes. We show that saliency and matching can be rigorously derived from our representation as the principal eigenvector and the smoothed Laplacian eigenvectors respectively. Learning the representation jointly allows matching to improve the deformation-invariance of saliency while allowing saliency to improve the feature localization of matching. To parameterize the representation from a mesh, we also propose a deep recurrent neural network (RNN) for effectively integrating multi-scale shape features and a soft-thresholding operator for adaptively enhancing the sparsity of saliency. Results show that by jointly learning from a pair of saliency and matching datasets, matching improves the accuracy of detected salient regions on meshes, which is especially obvious for small-scale saliency datasets, such as those having one to two meshes. At the same time, saliency improves the accuracy of shape matchings among meshes with reduced matching errors on surfaces. \n",
      "author:  Hu, Shanfeng and Shum, Hubert P. H. and Aslam, Nauman and Li, Frederick W. B. and Liang, Xiaohui\n",
      "year:  2020\n",
      "type:  JOUR\n",
      "journal/booktitle:  IEEE Transactions on Multimedia\n",
      "volume:  22\n",
      "number of pages:  15\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/TMM.2019.2952983\n",
      "ISSN:  19410077\n",
      "citation:  7\n",
      "Impact Factor:  6.513\n",
      "——————————————————————————————————————\n",
      "title:  Sparse Metric-based Mesh Saliency\n",
      "abstract:  In this paper, we propose an accurate and robust approach to salient region detection for 3D polygonal surface meshes. The salient regions of a mesh are those that geometrically stand out from their contexts and therefore are semantically important for geometry processing and shape analysis. However, a suitable definition of region contexts for saliency detection remains elusive in the field, and the previous methods fail to produce saliency maps that agree well with human annotations. We address these issues by computing saliency in a global manner and enforcing sparsity for more accurate saliency detection. Specifically, we represent the geometry of a mesh using a metric that globally en- codes the shape distances between every pair of local regions. We then propose a sparsity-enforcing rarity optimization problem, solving which allows us to obtain a compact set of salient regions globally distinct from each other. We build a perceptually motivated 3D eye fixation dataset and use a large-scale Schelling saliency dataset for extensive benchmarking of saliency detection methods. The results show that our computed saliency maps are closer to the ground-truth. To showcase the usefulness of our saliency maps for geometry processing, we apply them to feature point localization and achieve higher accuracy compared to established feature detectors.\n",
      "author:  Hu, Shanfeng and Liang, Xiaohui and Shum, Hubert P. H. and Li, Frederick W. B. and Aslam, Nauman\n",
      "year:  2020\n",
      "type:  JOUR\n",
      "journal/booktitle:  Neurocomputing\n",
      "volume:  400\n",
      "number of pages:  13\n",
      "publisher:  Elsevier\n",
      "DOI:  10.1016/j.neucom.2020.02.106\n",
      "ISSN:  09252312\n",
      "citation:  1\n",
      "Impact Factor:  5.719\n",
      "——————————————————————————————————————\n",
      "title:  A New Method to Evaluate the Dynamic Air Gap Thickness and Garment Sliding of Virtual Clothes During Walking\n",
      "abstract:  With the development of e-shopping, there is a significant growth in clothing purchases online. However, the virtual clothing fit evaluation is still under-researched. In the literature, the thickness of the air layer between the human body and clothes is a dominant geometric indicator to evaluate the clothing fit. However, such an approach has only been applied to the stationary positions of the manikin/human body. Physical indicators such as the pressure/tension of a virtual garment fitted on the virtual body in a continuous motion are also proposed for clothing fit evaluation. Both geometric and physical evaluations do not consider the interaction of the garment with body e.g. sliding of the garment along the human body. In this study, a new framework is proposed to automatically determine the dynamic air gap thickness. First, the dynamic dressed character sequence is simulated in a 3D clothing software via importing the body parameters, cloth parameters and a walking motion. Second, a cost function is defined to convert the garment in the previous frame to the local coordinate of the next frame. The dynamic air gap thickness between clothes and the human body is determined. Third, a new metric called 3D garment vector field (3DGVF) is proposed to represent the movement flow of the dynamic virtual garment, whose directional changes are calculated by cosine similarity. Experimental results show that our method is more sensitive to the small air gap thickness changes compared with start-of-the-arts, allowing it to more effectively evaluate clothing fit in a virtual environment.\n",
      "author:  Hu, Pengpeng and Ho, Edmond S. L. and Aslam, Nauman and Komura, Taku and Shum, Hubert P. H.\n",
      "year:  2019\n",
      "type:  JOUR\n",
      "journal/booktitle:  Textile Research Journal\n",
      "volume:  89\n",
      "number of pages:  14\n",
      "publisher:  SAGE\n",
      "DOI:  10.1177/0040517519826930\n",
      "ISSN:   \n",
      "citation:  7\n",
      "Impact Factor:  1.820\n",
      "——————————————————————————————————————\n",
      "title:  DSPP: Deep Shape and Pose Priors of Humans\n",
      "abstract:  The prior knowledge of real human body shapes and poses is fundamental in computer games and animation (e.g. performance capture). Linear subspaces such as the popular SMPL model have a limited capacity to represent the large geometric variations of human shapes and poses. What is worse is that random sampling from them often produces non-realistic humans because the distribution of real humans is more likely to concentrate on a non-linear manifold instead of the full subspace. Towards this problem, we propose to learn human shape and pose manifolds using a more powerful deep generator network, which is trained to produce samples that cannot be distinguished from real humans by a deep discriminator network. In contrast to previous work that learn both the generator and discriminator in the original geometry spaces, we learn them in the more representative latent spaces discovered by a shape and a pose auto-encoder network respectively. Random sampling from our priors produces higher-quality human shapes and poses. The capacity of our priors is best applied to applications such as virtual human synthesis in games.\n",
      "author:  Hu, Shanfeng and Shum, Hubert P. H. and Mucherino, Antonio\n",
      "year:  2019\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2019 International Conference on Motion in Games\n",
      "volume:   \n",
      "number of pages:  6\n",
      "publisher:  ACM\n",
      "DOI:  10.1145/3359566.3360051\n",
      "ISSN:  9781450369947\n",
      "citation:  0\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Stable Hand Pose Estimation under Tremor via Graph Neural Network\n",
      "abstract:  Hand pose estimation, which predicts the spatial location of hand joints, is a fundamental task in VR/AR applications. Although existing methods can recover hand pose competently, the tremor issue occurring in hand motion has not been completely solved. Tremor is an involuntary motion accompanied by a desired gesture or hand motion, leading to hand pose that deviates from user’s intentions. Considering the characteristic of tremor motion, we present a novel Graph Neural Network for stable 3D hand pose estimation. The input is depth images. The constraint adjacency matrix is devised in Graph Neural Network for dynamically adjusting the topology of a hand graph during message passing and aggregation. Firstly, since there are rich potential constraints among hand joints, we utilize the constraint adjacency matrix to mine the suitable topology, modeling spatial-temporal constraints of joints and outputting the precise tremor hand pose as the pre-estimation result. Then, for obtaining a stable hand pose, we provide a tremor compensation module based on the constraint adjacency matrix, which exploits the constraint between control points and tremor hand pose. Concretely, the control points represented the voluntary motion are employed as constraints to edit the tremor hand pose. Our extensive quantitative and qualitative experiments show that the proposed method has achieved decent performance for 3D tremor hand pose estimation. \n",
      "author:  Leng, Zhiying and Chen, Jiaying and Shum, Hubert P. H. and Li, Frederick W. B. and Liang, Xiaohui\n",
      "year:  2021\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2021 IEEE Conference on Virtual Reality and 3D User Interfaces\n",
      "volume:   \n",
      "number of pages:  9\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/VR50410.2021.00044\n",
      "ISSN:  26425254\n",
      "citation:  1\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:  STGAE: Spatial Temporal Graph Auto-encoder for Hand Motion Denoising\n",
      "abstract:  Hand object interaction in mixed reality (MR) relies on the accurate tracking and estimation of human hands, which provide users with a sense of immersion. However, raw captured hand motion data always contains errors such as joints occlusion, dislocation, high-frequency noise, and involuntary jitter. Denoising and obtaining the hand motion data consistent with the user’s intention are of the utmost importance to enhance the interactive experience in MR. To this end, we propose an end-to-end method for hand motion denoising using the spatial-temporal graph auto-encoder (STGAE). The spatial and temporal patterns are recognized simultaneously by constructing the consecutive hand joint sequence as a spatial-temporal graph. Considering the complexity of the articulated hand structure, a simple yet effective partition strategy is proposed to model the physic-connected and symmetry-connected relationships. Graph convolution is applied to extract structural constraints of the hand, and a self-attention mechanism is to adjust the graph topology dynamically. Combining graph convolution and temporal convolution, a fundamental graph encoder or decoder block is proposed. We finally establish the hourglass residual auto-encoder to learn a manifold projection operation and a corresponding inverse projection through stacking these blocks. In this work, the proposed framework has been successfully used in hand motion data denoising with preserving structural constraints between joints. Extensive quantitative and qualitative experiments show that the proposed method has achieved better performance than the state-of-the-art approaches.\n",
      "author:  Zhou, Kanglei and Chen, Jiaying and Shum, Hubert P. H. and Li, Frederick W. B. and Liang, Xiaohui\n",
      "year:  2021\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2021 IEEE International Symposium on Mixed and Augmented Reality\n",
      "volume:   \n",
      "number of pages:   \n",
      "publisher:  IEEE\n",
      "DOI:   \n",
      "ISSN:   \n",
      "citation:  0\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Resolving Hand-Object Occlusion for Mixed Reality with Joint Deep Learning and Model Optimization\n",
      "abstract:  By overlaying virtual imagery onto the real world, mixed reality facilitates diverse applications and has drawn increasing attention. Enhancing physical in-hand objects with a virtual appearance is a key component for many applications that require users to interact with tools such as surgery simulations. However, due to complex hand articulations and severe hand-object occlusions, resolving occlusions in hand-object interactions is a challenging topic. Traditional tracking-based approaches are limited by strong ambiguities from occlusions and changing shapes, while reconstruction-based methods show poor capability of handling dynamic scenes. In this paper, we propose a novel real-time optimization system to resolve hand-object occlusions by spatially reconstructing the scene with estimated hand joints and masks. To acquire accurate results, we propose a joint learning process that shares information between two models and jointly estimates hand poses and semantic segmentation. To facilitate the joint learning system and improve its accuracy under occlusions, we propose an occlusion-aware RGB-D hand dataset that mitigates the ambiguity through precise annotations and photorealistic appearance. Evaluations show more consistent overlays compared to literature, and a user study verifies a more realistic experience.\n",
      "author:  Feng, Qi and Shum, Hubert P. H. and Morishima, Shigeo\n",
      "year:  2020\n",
      "type:  JOUR\n",
      "journal/booktitle:  Computer Animation and Virtual Worlds\n",
      "volume:  31\n",
      "number of pages:  12\n",
      "publisher:  John Wiley and Sons Ltd.\n",
      "DOI:  10.1002/cav.1956\n",
      "ISSN:   \n",
      "citation:  0\n",
      "Impact Factor:  1.020\n",
      "——————————————————————————————————————\n",
      "title:  NETIVAR: NETwork Information Visualization based on Augmented Reality\n",
      "abstract:  Connecting network cables to network switches is a time-consuming and inefficient task, and requires extensive documentation and preparation beforehand to ensure no service faults are encountered by the users. In this paper, a new AR smartphone application that overlays network switch information over the user’s vision is designed and developed for real working environment to increase user’s efficiency in working with a network switch. Specifically, the prototype of the AR App is developed on the Android platform using both the Unity game engine and Vuforia AR library and connecting to the network switch to retrieve network information through telnet. By using the camera on the smartphone for capturing the visual information from the working environment, i.e. the network switch in this App, the network switch information such as speed, types, etc. will be overlaid on each port on the smartphone screen. A user study was conducted to evaluate the effectiveness of the AR App to assist users in performing network tasks. In particular, participants were tasked with connecting switchports to a patch panel to match up corresponding configurations. After three tests, it was found that the times for completion and mistakes made were reduced in the final test when compared to the first. This highlights the positive effects of the application in improving the user’s efficiency. \n",
      "author:  Flinton, Christopher and Anderson, Philip and Shum, Hubert P. H. and Ho, Edmond S. L.\n",
      "year:  2018\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2018 International Conference on Software Knowledge Information Management and Applications\n",
      "volume:   \n",
      "number of pages:  8\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/SKIMA.2018.8631530\n",
      "ISSN:  25733214\n",
      "citation:  1\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Patient Assessment Assistant Using Augmented Reality\n",
      "abstract:  Facial symmetry and averageness are key components in quantifying the perception of beauty. In this study, a prototype Augmented Reality (AR) tool is developed on Android OS, to assist plastic surgeons and patients in objectively assessing facial symmetry when planning reconstructive surgical procedures. Speci cally, the tool overlays 4 types of measurements and guidelines over a live video stream to provide the users with useful information interactively. The measurements are computed from the tracked facial landmarks at run-time.\n",
      "author:  Ho, Edmond S. L. and McCay, Kevin D. and Shum, Hubert P. H. and Yang, Longzhi and Sainsbury, David and Hodgkinson, Peter\n",
      "year:  2018\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2018 UKChina Newton Fund Researcher Links Workshop Health and Wellbeing Through VR and AR\n",
      "volume:   \n",
      "number of pages:  9\n",
      "publisher:   \n",
      "DOI:   \n",
      "ISSN:   \n",
      "citation:  0\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Occlusion for 3D Object Manipulation with Hands in Augmented Reality\n",
      "abstract:  By dynamically enhancing the reality with a wide range of applications, augmented reality has been drawing growing attention during recent years. Due to the need to interact with virtual objects, hand-object interaction has become a critical element in these applications. By enhancing existing objects with virtual visual appearance, tactile feedback can be provided to the user as well. However, to deal with problems of augmenting real objects with virtual data, a correct visual representation is needed. This becomes even more critical when the interaction between virtual and real objects happens. Incorrect occlusion information can be fatal to an immersive augmented reality experience. In this paper, we propose an innovative approach to handle the occlusion of augmented 3D object manipulation with hands in real-time by exploiting the nature of hand pose combined with tracking-based and model-based methods for estimating occlusion, to achieve a complete augmented reality experience without the necessities of heavy computations, complex manual segmentation processes or wearing special gloves.\n",
      "author:  Feng, Qi and Shum, Hubert P. H. and Morishima, Shigeo\n",
      "year:  2018\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2018 Meeting on Image Recognition and Understanding\n",
      "volume:   \n",
      "number of pages:  4\n",
      "publisher:   \n",
      "DOI:   \n",
      "ISSN:   \n",
      "citation:  1\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:  Resolving Occlusion for 3D Object Manipulation with Hands in Mixed Reality\n",
      "abstract:  Due to the need to interact with virtual objects, the hand-object interaction has become an important element in mixed reality (MR) applications. In this paper, we propose a novel approach to handle the occlusion of augmented 3D object manipulation with hands by exploiting the nature of hand poses combined with tracking-based and model-based methods, to achieve a complete mixed reality experience without necessities of heavy computations, complex manual segmentation processes or wearing special gloves. The experimental results show a frame rate faster than real-time and a great accuracy of rendered virtual appearances, and a user study verifies a more immersive experience compared to past approaches. We believe that the proposed method can improve a wide range of mixed reality applications that involve hand-object interactions.\n",
      "author:  Feng, Qi and Shum, Hubert P. H. and Morishima, Shigeo\n",
      "year:  2018\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2018 ACM Symposium on Virtual Reality Software and Technology\n",
      "volume:   \n",
      "number of pages:  2\n",
      "publisher:  ACM\n",
      "DOI:  10.1145/3281505.3283390\n",
      "ISSN:  9.78E+12\n",
      "citation:  8\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Fast Accelerometer-Based Motion Recognition with a Dual Buffer Framework\n",
      "abstract:  The low-cost gyro-accelerometer based controllers have opened up a potential of using 3D computer games for serious applications such as sports and vocational training. Previous games apply simple template matching for the motion recognition which suffers from poor accuracy and time lag. In order to cope with these problems, we propose a novel dual buffer approach that dramatically increases the recognition rate and shortens the time lag for motion recognition. The system first recognizes the user control signal with a small buffer to minimize the time lag. When more signals arrive from the sensor, an elaborate recognition is performed, and the previously recognized action is switched if necessary. Using boxing as an example, we show that we can control a virtual character to perform 13 different actions using a buffer size of one-tenth of a second. Since our system is computationally inexpensive, it can be used in game consoles. As it is accurate and responsive, it can also be applied for serious sport training.\n",
      "author:  Shum, Hubert P. H. and Komura, Taku and Takagi, Shu\n",
      "year:  2011\n",
      "type:  JOUR\n",
      "journal/booktitle:  The International Journal of Virtual Reality\n",
      "volume:  10\n",
      "number of pages:  8\n",
      "publisher:  IPI Press\n",
      "DOI:  10.20870/IJVR.2011.10.3.2831\n",
      "ISSN:   \n",
      "citation:  7\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  LMZMPM: Local Modified Zernike Moment Per-unit Mass for Robust Human Face Recognition\n",
      "abstract:  In this work, we proposed a novel method, called Local Modified Zernike Moment per unit Mass (LMZMPM), for face recognition, which is invariant to illumination, scaling, noise, in-plane rotation, and translation, along with other orthogonal and inherent properties of the Zernike Moments (ZMs). The proposed LMZMPM is computed for each pixel in a neighborhood of size 3 x 3, and then considers the complex tuple that contains both the phase and magnitude coefficients of LMZMPM as the extracted features. As it contains both the phase and the magnitude components of the complex feature, it has more information about the image and thus preserves both the edge and structural information. We also propose a hybrid similarity measure, combining the Jaccard Similarity with the L1 distance, and applied to the extracted feature set for classification. The feasibility of the proposed LMZMPM technique on varying illumination has been evaluated on the CMU-PIE and the extended Yale B databases with an average Rank-1 Recognition (R1R) accuracy of 99.8% and 98.66% respectively. To assess the reliability of the method with variations in noise, rotation, scaling, and translation, we evaluate it on the AR database and obtain an average R1R higher than that of recent state-of-the-art methods. The proposed method shows a very high recognition rate on Heterogeneous Face Recognition as well, with 100% on CUFS, and 98.80% on CASIA-HFB. \n",
      "author:  Kar, Arindam and Pramanik, Sourav and Chakraborty, Arghya and Bhattacharjee, Debotosh and Ho, Edmond S. L. and Shum, Hubert P. H.\n",
      "year:  2021\n",
      "type:  JOUR\n",
      "journal/booktitle:  IEEE Transactions on Information Forensics and Security\n",
      "volume:  16\n",
      "number of pages:  15\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/TIFS.2020.3015552\n",
      "ISSN:  15566021\n",
      "citation:  9\n",
      "Impact Factor:  7.178\n",
      "——————————————————————————————————————\n",
      "title:  Two-Stage Human Verification using HandCAPTCHA and Anti-Spoofed Finger Biometrics with Feature Selection\n",
      "abstract:  This paper presents a human verification scheme in two independent stages to overcome the vulnerabilities of attacks and to enhance security. At the first stage, a hand image-based CAPTCHA (HandCAPTCHA) is tested to avert automated bot-attacks on the subsequent biometric stage. In the next stage, finger biometric verification of a legitimate user is performed with presentation attack detection (PAD) using the real hand images of the person who has passed a random HandCAPTCHA challenge. The electronic screen-based PAD is tested using image quality metrics. After this spoofing detection, geometric features are extracted from the four fingers (excluding the thumb) of real users. A modified forward-backward (M-FoBa) algorithm is devised to select relevant features for biometric authentication. The experiments are performed on the Bogazici University (BU) and the IIT-Delhi (IITD) hand databases using the k-nearest neighbor and random forest classifiers. The average accuracy of the correct HandCAPTCHA solution is 98.5%, and the false accept rate of a bot is 1.23%. The PAD is tested on 255 subjects of BU, and the best average error is 0%. The finger biometric identification accuracy of 98% and an equal error rate (EER) of 6.5% have been achieved for 500 subjects of the BU. For 200 subjects of the IITD, 99.5% identification accuracy, and 5.18% EER are obtained.\n",
      "author:  Bera, Asish and Bhattacharjee, Debotosh and Shum, Hubert P. H.\n",
      "year:  2021\n",
      "type:  JOUR\n",
      "journal/booktitle:  Expert Systems with Applications\n",
      "volume:  171\n",
      "number of pages:  18\n",
      "publisher:  Elsevier\n",
      "DOI:  10.1016/j.eswa.2021.114583\n",
      "ISSN:  09574174\n",
      "citation:  3\n",
      "Impact Factor:  6.954\n",
      "——————————————————————————————————————\n",
      "title:  Spoofing Detection on Hand Images Using Quality Assessment\n",
      "abstract:  Recent research on biometrics focuses on achieving a high success rate of authentication and addressing the concern of various spoofing attacks. Although hand geometry recognition provides adequate security over unauthorized access, it is susceptible to presentation attack. This paper presents an anti-spoofing method toward hand biometrics. A presentation attack detection approach is addressed by assessing the visual quality of genuine and fake hand images. A threshold-based gradient magnitude similarity quality metric is proposed to discriminate between the real and spoofed hand samples. The visual hand images of 255 subjects from the Bogazici University hand database are considered as original samples. Correspondingly, from each genuine sample, we acquire a forged image using a Canon EOS 700D camera. Such fake hand images with natural degradation are considered for electronic screen display based spoofing attack detection. Furthermore, we create another fake hand dataset with artificial degradation by introducing additional Gaussian blur, salt and pepper, and speckle noises to original images. Ten quality metrics are measured from each sample for classification between original and fake hand image. The classification experiments are performed using the k-nearest neighbors, random forest, and support vector machine classifiers, as well as deep convolutional neural networks. The proposed gradient similarity-based quality metric achieves 1.5% average classification error using the k-nearest neighbors and random forest classifiers. An average classification error of 2.5% is obtained using the baseline evaluation with the MobileNetV2 deep network for discriminating original and different types of fake hand samples. \n",
      "author:  Bera, Asish and Dey, Ratnadeep and Bhattacharjee, Debotosh and Nasipuri, Mita and Shum, Hubert P. H.\n",
      "year:  2021\n",
      "type:  JOUR\n",
      "journal/booktitle:  Multimedia Tools and Applications\n",
      "volume:  80\n",
      "number of pages:  24\n",
      "publisher:  Springer\n",
      "DOI:  10.1007/s1104202110976z\n",
      "ISSN:  15737721\n",
      "citation:  1\n",
      "Impact Factor:  2.757\n",
      "——————————————————————————————————————\n",
      "title:  Unifying Person and Vehicle Re-identification\n",
      "abstract:  Person and vehicle re-identification (re-ID) are important challenges for the analysis of the burgeoning collection of urban surveillance videos. To efficiently evaluate such videos, which are populated with both vehicles and pedestrians, it would be preferable to have one unified framework with effective performance across both domains. Unfortunately, due to the contrasting composition of humans and vehicles, no architecture has yet been established that can adequately perform both tasks. We release a Person and Vehicle Unified Data Set (PVUD) comprising of both pedestrians and vehicles from popular existing re-ID data sets, in order to better model the data that we would expect to find in the real world. We exploit the generalisation ability of metric learning to propose a re-ID framework that can learn to re-identify humans and vehicles simultaneously. We design our network, MidTriNet, to harness the power of mid-level features to develop better representations for the re-ID tasks. We help the system to handle mixed data by appending unification terms with additional hard negative and hard positive mining to MidTriNet. We attain comparable accuracy training on PVUD to training on the comprising data sets separately, supporting the system’s generalisation power. To further demonstrate the effectiveness of our framework, we also obtain results better than, or competitive with, the state-of-the-art on each of the Market-1501, CUHK03, VehicleID and VeRi data sets. \n",
      "author:  Organisciak, Daniel and Sakkos, Dimitrios and Ho, Edmond S. L. and Aslam, Nauman and Shum, Hubert P. H.\n",
      "year:  2020\n",
      "type:  JOUR\n",
      "journal/booktitle:  IEEE Access\n",
      "volume:  8\n",
      "number of pages:  12\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/ACCESS.2020.3004092\n",
      "ISSN:  21693536\n",
      "citation:  8\n",
      "Impact Factor:  3.367\n",
      "——————————————————————————————————————\n",
      "title:  A Privacy-Preserving Efficient Location-Sharing Scheme for Mobile Online Social Network Applications\n",
      "abstract:  The rapid development of mobile internet technology and the better availability of GPS have made mobile online social networks (mOSNs) more popular than traditional online social networks (OSNs) over the last few years. They necessitate fundamental social operations such as establishing friend relationship, location sharing among friends, and providing location-based services. As a consequence, security and privacy issues demands the utmost importance to mOSNs users. The first stream of existing solutions adopts two different servers to store locations-based and social network-based information separately, thereby sustaining large storage and communication overhead. The second stream of solutions aims at integrating the social network server and the location-based server into a single entity. However, as these approaches exploit only one single server, they may face several performance issues related to server bottlenecks. Moreover, such schemes are found to be vulnerable to various active and passive security attacks. In this paper, we propose a privacy preserving, secure and efficient location sharing scheme for mOSNs, which shows both efficiency and flexibility in the location update, sharing, and query of social friends and social strangers. The security of the proposed scheme is validated using random oracle based formal security proof and Burrows-Abadi-Needham (BAN) logic based authentication proof, followed by informal security analysis. Additionally, we have used ProVerif 1.93 to verify the security of the system. The efficiency and practicability of the proposed scheme are demonstrated through experimental implementation and evaluation.\n",
      "author:  Bhattacharya, Munmun and Roy, Sandip and Mistry, Kamlesh and Shum, Hubert P. H. and Chattopadhyay, Samiran\n",
      "year:  2020\n",
      "type:  JOUR\n",
      "journal/booktitle:  IEEE Access\n",
      "volume:   \n",
      "number of pages:  22\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/ACCESS.2020.3043621\n",
      "ISSN:  21693536\n",
      "citation:  1\n",
      "Impact Factor:  3.367\n",
      "——————————————————————————————————————\n",
      "title:  Multiview Discriminative Marginal Metric Learning for Makeup Face Verification\n",
      "abstract:  Makeup face verification in the wild is an important research problem in computer vision for its popularization in real-world. However, little e?ort has been made to tackle it. In this research, we first build a new database, i.e., Facial Beauty Database (FBD), which contains 17,866 paired facial images of 8,933 subjects without and with makeup in different real-world scenarios. To the best of our knowledge, FBD is the largest makeup face database to date compared with existing databases for facial makeup research. Moreover, we propose a new discriminative marginal metric learning (DMML) algorithm to deal with this problem in the wild. Inspired by the fact that interclass marginal faces are usually more discriminative than interclass nonmarginal faces in learning the discriminative metric space, we use the interclass marginal faces to depict the discriminative information. Simultaneously, we wish that those interclass marginal faces without makeup relations are separated from each other as far as possible, so that more discriminative information between facial images without and with makeup can be exploited for verification. Furthermore, since multiple features could provide comprehensive information in describing the facial representations from diverse points of view and extract more informative cues from facial images, we introduce a multiview discriminative marginal metric learning (MDMML) algorithm by effectively learning a robust metric space such that multiple features from different points of view can be integrated to effectively enhance the performance of makeup face verification. Experimental results on two real-world makeup face databases are utilized to show the effectiveness of our method and the possibility of verifying the makeup relations from facial images in real-world. \n",
      "author:  Zhang, Lining and Shum, Hubert P. H. and Liu, Li and Guo, Guodong and Shao, Ling\n",
      "year:  2019\n",
      "type:  JOUR\n",
      "journal/booktitle:  Neurocomputing\n",
      "volume:  333\n",
      "number of pages:  12\n",
      "publisher:  Elsevier\n",
      "DOI:  10.1016/j.neucom.2018.12.003\n",
      "ISSN:  09252312\n",
      "citation:  21\n",
      "Impact Factor:  5.719\n",
      "——————————————————————————————————————\n",
      "title:  A Secure Authentication Protocol for Multi-server-based e-Healthcare using a Fuzzy Commitment Scheme\n",
      "abstract:  Smart card-based remote authentication schemes are widely used in multi-medical-server-based telecare medicine information systems (TMIS). Biometric is one of the most trustworthy authenticators, and is presently being advocated to use in the remote authentication of TMIS. However, most of the existing TMISs consider a single-server-environment-based authentication system. Therefore, patients need to register and log into every server separately for different services. Furthermore, these schemes do not employ error correction technique to remove the errors from biometric data. Also, biometrics are inherent and demand diversification to generate a revocable template from inherent biometric data. In this paper, we propose a mutual authentication and key agreement scheme for a multi-medical server environment to overcome the limitations of the existing schemes. In the proposed scheme, a cancelable transformation of the raw biometric data is used to provide the privacy and the diversification of biometric data. The errors of the biometric data are corrected with error-correction techniques under the fuzzy commitment mechanism. Formal security analysis using the widely accepted Real-Or-Random (ROR) model, the Burrows-Abadi- Needham (BAN) logic and the Automated Validation of Internet Security Protocols and Applications (AVISPA) tool concludes that the proposed scheme is safe against known attacks. We also compare the computation and communication costs of our scheme to evaluate the performance with the others.\n",
      "author:  Barman, Subhas and Shum, Hubert P. H. and Chattopadhyay, Samiran and Samanta, Debasis\n",
      "year:  2019\n",
      "type:  JOUR\n",
      "journal/booktitle:  IEEE Access\n",
      "volume:  7\n",
      "number of pages:  18\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/ACCESS.2019.2893185\n",
      "ISSN:  21693536\n",
      "citation:  21\n",
      "Impact Factor:  3.367\n",
      "——————————————————————————————————————\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:  Triplet Loss with Channel Attention for Person Re-identification\n",
      "abstract:  The triplet loss function has seen extensive use within person re-identification. Most works focus on either improving the mining algorithm or adding new terms to the loss function itself. Our work instead concentrates on two other core components of the triplet loss that have been under-researched. First, we improve the standard Euclidean distance with dynamic weights, which are selected based on the standard deviation of features across the batch. Second, we exploit channel attention via a squeeze and excitation unit in the backbone model to emphasise important features throughout all layers of the model. This ensures that the output feature vector is a better representation of the image, and is also more suitable to use within our dynamically weighted Euclidean distance function. We demonstrate that our alterations provide significant performance improvement across popular reidentification data sets, including almost 10% mAP improvement on the CUHK03 data set. The proposed model attains results competitive with many state-of-the-art person re-identification models.\n",
      "author:  Organisciak, Daniel and Riachy, Chirine and Aslam, Nauman and Shum, Hubert P. H.\n",
      "year:  2019\n",
      "type:  JOUR\n",
      "journal/booktitle:  Journal of WSCG\n",
      "volume:  27\n",
      "number of pages:  9\n",
      "publisher:   \n",
      "DOI:  10.24132/JWSCG.2019.27.2.9\n",
      "ISSN:  12136972\n",
      "citation:  7\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Facial Reshaping Operator for Controllable Face Beautification\n",
      "abstract:  Posting attractive facial photos is part of everyday life in the social media era. Motivated by the demand, we propose a lightweight method to automatically and efficiently beautify the shapes of both portrait and non-portrait faces in photos, while allowing users to customize the beautification of individual facial features. Previous methods focus on the beautification of mostly frontal and neutral faces, without incorporating user controllability in the beautification process. To address these restrictions, we propose the Facial Reshaping Operator representation, which is affine-invariant, captures the pairwise geometric configuration of facial landmarks, and allows for efficient face beautification with the user-specified weights of individual facial parts. We also propose an unsupervised beautification method in the operator space of faces, where an input face is iteratively pulled towards a local nearby density mode with improved attractiveness. Our method distinguishes itself from the commercial beautification tools in that it mildly enhances facial shapes without altering makeups or complexions, which complements these tools that lack fine-grained control on the attractiveness of facial shapes for users. The experimental results show that our method improves facial shape attractiveness for a large range of poses and expressions, demonstrating the potential of applicability to photos seen on the social media such as Facebook and Instagram everyday. \n",
      "author:  Hu, Shanfeng and Shum, Hubert P. H. and Liang, Xiaohui and Li, Frederick W. B. and Aslam, Nauman\n",
      "year:  2021\n",
      "type:  JOUR\n",
      "journal/booktitle:  Expert Systems with Applications\n",
      "volume:  167\n",
      "number of pages:  13\n",
      "publisher:  Elsevier\n",
      "DOI:  10.1016/j.eswa.2020.114067\n",
      "ISSN:  09574174\n",
      "citation:  0\n",
      "Impact Factor:  6.954\n",
      "——————————————————————————————————————\n",
      "title:  Makeup Style Transfer on Low-quality Images with Weighted Multi-scale Attention\n",
      "abstract:  Facial makeup style transfer is an extremely challenging sub-field of image-to-image-translation. Due to this difficulty, state-of-the-art results are mostly reliant on the Face Parsing Algorithm, which segments a face into parts in order to easily extract makeup features. However, this algorithm can only work well on high-definition images where facial features can be accurately extracted. Faces in many real-world photos, such as those including a large background or multiple people, are typically of low-resolution, which considerably hinders state-of- the-art algorithms. In this paper, we propose an end-to-end holistic approach to effectively transfer makeup styles between two low-resolution images. The idea is built upon a novel weighted multi-scale spatial attention module, which identifies salient pixel regions on low-resolution images in multiple scales, and uses channel attention to determine the most effective attention map. This design provides two benefits: low-resolution images are usually blurry to different extents, so a multi-scale architecture can select the most effective convolution kernel size to implement spatial attention; makeup is applied on both a macro-level (foundation, fake tan) and a micro-level (eyeliner, lipstick) so different scales can excel in extracting different makeup features. We develop an Augmented CycleGAN network that embeds our attention modules at selected layers to most effectively transfer makeup. Our system is tested with the FBD data set, which consists of many low-resolution facial images, and demonstrate that it outperforms state-of-the-art methods, particularly in transferring makeup for blurry images and partially occluded images.\n",
      "author:  Organisciak, Daniel and Ho, Edmond S. L. and Shum, Hubert P. H.\n",
      "year:  2020\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2020 International Conference on Pattern Recognition\n",
      "volume:   \n",
      "number of pages:  8\n",
      "publisher:   \n",
      "DOI:  10.1109/ICPR48806.2021.9412604\n",
      "ISSN:   \n",
      "citation:  1\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Synthesizing Expressive Facial and Speech Animation by Text-to-IPA Translation with Emotion Control\n",
      "abstract:  Given the complexity of the human facial anatomy, animating facial expressions and lip movements for speech is a very time-consuming and tedious task. In this paper, a new text-to-animation framework for facial animation synthesis is proposed. The core idea is to improve the expressiveness of lip-sync animation by incorporating facial expressions in 3D animated characters. This idea is realized as a plug-in in Autodesk Maya, one of the most popular animation platforms in the industry, such that professional animators can effectively apply the method in their existing work. We evaluate the proposed system by conducting two sets of surveys, in which both novice and experienced users participate in the user study to provide feedback and evaluations from different perspectives. The results of the survey highlights the effectiveness of creating realistic facial animations with the use of emotion expressions. Video demos of the synthesized animations are available online at https://git.io/fx5U3\n",
      "author:  Stef, Andreea and Perera, Kaveen and Shum, Hubert P. H. and Ho, Edmond S. L.\n",
      "year:  2018\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2018 International Conference on Software Knowledge Information Management and Applications\n",
      "volume:   \n",
      "number of pages:  8\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/SKIMA.2018.8631536\n",
      "ISSN:  25733214\n",
      "citation:  2\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  PyTorch-based Implementation of Label-aware Graph Representation for Multi-class Trajectory Prediction\n",
      "abstract:  Trajectory Prediction under diverse patterns has attracted increasing attention in multiple real-world applications ranging from urban traffic analysis to human motion understanding, among which graph convolution network (GCN) is frequently adopted with its superior ability in modeling the complex trajectory interactions among multiple humans. In this work, we propose a python package by enhancing GCN with class label information of the trajectory, such that we can explicitly model not only human trajectories but also that of other road users such as vehicles. This is done by integrating a label-embedded graph with the existing graph structure in the standard graph convolution layer. The flexibility and the portability of the package also allow researchers to employ it under more general multi-class sequential prediction tasks.\n",
      "author:  Men, Qianhui and Shum, Hubert P. H.\n",
      "year:  2021\n",
      "type:  JOUR\n",
      "journal/booktitle:  Software Impacts\n",
      "volume:   \n",
      "number of pages:   \n",
      "publisher:  Elsevier\n",
      "DOI:  10.1016/j.simpa.2021.100201\n",
      "ISSN:  26659638\n",
      "citation:  0\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:  Semantics-STGCNN: A Semantics-guided Spatial Temporal Graph Convolutional Network for Multi-class Trajectory Prediction\n",
      "abstract:  Predicting the movement trajectories of multiple classes of road users in real-world scenarios is a challenging task due to the diverse trajectory patterns. While recent works of pedestrian trajectory prediction successfully modelled the influence of surrounding neighbours based on the relative distances, they are ineffective on multi-class trajectory prediction. This is because they ignore the impact of the implicit correlations between different types of road users on the trajectory to be predicted - for example, a nearby pedestrian has a different level of influence from a nearby car. In this paper, we propose to introduce class information into a graph convolutional neural network to better predict the trajectory of an individual. We embed the class labels of the surrounding objects into the label adjacency matrix (LAM), which is combined with the velocity-based adjacency matrix (VAM) comprised of the objects' velocity, thereby generating a semantics-guided graph adjacency (SAM). SAM effectively models semantic information with trainable parameters to automatically learn the embedded label features that will contribute to the fixed velocity-based trajectory. Such information of spatial and temporal dependencies is passed to a graph convolutional and temporal convolutional network to estimate the predicted trajectory distributions. We further propose new metrics, known as Average2 Displacement Error (aADE) and Average Final Displacement Error (aFDE), that assess network accuracy more accurately. We call our framework Semantics-STGCNN. It consistently shows superior performance to the state-of-the-arts in existing and the newly proposed metrics.\n",
      "author:  Rainbow, Ben and Men, Qianhui and Shum, Hubert P. H.\n",
      "year:  2021\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2021 IEEE International Conference on Systems, Man, and Cybernetics\n",
      "volume:   \n",
      "number of pages:   \n",
      "publisher:  IEEE\n",
      "DOI:   \n",
      "ISSN:   \n",
      "citation:  1\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Formation Control for UAVs Using a Flux Guided Approach\n",
      "abstract:  While multiple studies have proposed methods for the formation control of unmanned aerial vehicles (UAV), the trajectories generated are generally unsuitable for tracking targets where the optimum coverage of the target by the formation is required at all times. We propose a path planning approach called the Flux Guided (FG) method, which generates collision-free trajectories while maximising the coverage of one or more targets. We show that by reformulating an existing least-squares flux minimisation problem as a constrained optimisation problem, the paths obtained are 1.5X shorter and track directly toward the target. Also, we demonstrate that the scale of the formation can be controlled during flight, and that this feature can be used to track multiple scattered targets. The method is highly scalable since the planning algorithm is only required for a sub-set of UAVs on the open boundary of the formation's surface. Finally, through simulating a 3d dynamic particle system that tracks the desired trajectories using a PID controller, we show that the resulting trajectories after time-optimal parameterisation are suitable for robotic controls.\n",
      "author:  Hartley, John and Shum, Hubert P. H. and Ho, Edmond S. L. and Wang, He and Ramamoorthy, Subramanian\n",
      "year:  2021\n",
      "type:  Preprint\n",
      "journal/booktitle:   \n",
      "volume:   \n",
      "number of pages:  8\n",
      "publisher:   \n",
      "DOI:   \n",
      "ISSN:   \n",
      "citation:  0\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Data-Driven Crowd Motion Control with Multi-touch Gestures\n",
      "abstract:  Controlling a crowd using multi-touch devices appeals to the computer games and animation industries, as such devices provide a high dimensional control signal that can effectively define the crowd formation and movement. However, existing works relying on pre-defined control schemes require the users to learn a scheme that may not be intuitive. We propose a data-driven gesture-based crowd control system, in which the control scheme is learned from example gestures provided by different users. In particular, we build a database with pairwise samples of gestures and crowd motions. To effectively generalize the gesture style of different users, such as the use of different numbers of fingers, we propose a set of gesture features for representing a set of hand gesture trajectories. Similarly, to represent crowd motion trajectories of different numbers of characters over time, we propose a set of crowd motion features that are extracted from a Gaussian mixture model. Given a run-time gesture, our system extracts the K nearest gestures from the database and interpolates the corresponding crowd motions in order to generate the run-time control. Our system is accurate and efficient, making it suitable for real-time applications such as real-time strategy games and interactive animation controls.\n",
      "author:  Shen, Yijun and Henry, Joseph and Wang, He and Ho, Edmond S. L. and Komura, Taku and Shum, Hubert P. H.\n",
      "year:  2018\n",
      "type:  JOUR\n",
      "journal/booktitle:  Computer Graphics Forum\n",
      "volume:  37\n",
      "number of pages:  14\n",
      "publisher:  John Wiley and Sons Ltd.\n",
      "DOI:  10.1111/cgf.13333\n",
      "ISSN:  14678659\n",
      "citation:  4\n",
      "Impact Factor:  2.078\n",
      "——————————————————————————————————————\n",
      "title:  Unsupervised Abnormal Behaviour Detection with Overhead Crowd Video\n",
      "abstract:  Due to the increasing threat of terrorism, it has become more and more important to detect abnormal behaviour in public areas. In this paper, we introduce a system to identify pedestrians with abnormal movement trajectories in a scene using a data-driven approach. Our system includes two parts. The first part is an interactive tool that takes an overhead video as an input and tracks the pedestrians in a semi-automatic manner. The second part is a data-driven abnormal trajectories detection algorithm, which applies iterative k-means clustering to find out possible paths in the scene and thereby identifies those that do not fit well in any paths. Since the system requires only RGB video, it is compatible with most of the closed-circuit television (CCTV) systems used for security monitoring. Furthermore, the training of the abnormal trajectories detection algorithm is unsupervised and fully automatic. It means that the system can be deployed into a new location without manual parameter tuning and training data annotations. The system can be applied in indoor and outdoor environments and is best for automatic security monitoring.\n",
      "author:  Xu, Shoujiang and Ho, Edmond S. L. and Aslam, Nauman and Shum, Hubert P. H.\n",
      "year:  2017\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2017 International Conference on Software Knowledge Information Management and Applications\n",
      "volume:   \n",
      "number of pages:  6\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/SKIMA.2017.8294092\n",
      "ISSN:  25733214\n",
      "citation:  2\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Coordinated Crowd Simulation with Topological Scene Analysis\n",
      "abstract:  This paper proposes a new algorithm to produce globally coordinated crowds in an environment with multiple paths and obstacles. Simple greedy crowd control methods easily lead to congestion at bottlenecks within scenes, as the characters do not cooperate with one another. In computer animation, this problem degrades crowd quality especially when ordered behaviour is needed, such as soldiers marching towards a castle. Similarly, in applications such as real-time strategy games, this often causes player frustration, as the crowd will not move as efficiently as it should. Also, planning of building would usually require visualization of ordered evacuation to maximize the flow. Planning such globally coordinated crowd movement is usually labour intensive. Here, we propose a simple solution that is easy to use and efficient in computation. First, we compute the harmonic field of the environment, taking into account the starting points, goals and obstacles. Based on the field, we represent the topology of the environment using a Reeb Graph, and calculate the maximum capacity for each path in the graph. With the harmonic field and the Reeb Graph, path planning of crowd can be performed using a lightweight algorithm, such that any blocking of one another??s paths is minimized. Comparing to previous methods, our system can synthesize globally coordinated crowd with smooth and efficient movement. It also enables control of the crowd with high-level parameters such as the degree of cooperation and congestion. Finally, the method is scalable to thousands of characters with minimal impact to computation time. It is best applied in interactive crowd synthesis systems such as animation designs and real-time strategy games. \n",
      "author:  Barnett, Adam and Shum, Hubert P. H. and Komura, Taku\n",
      "year:  2016\n",
      "type:  JOUR\n",
      "journal/booktitle:  Computer Graphics Forum\n",
      "volume:  35\n",
      "number of pages:  13\n",
      "publisher:  John Wiley and Sons Ltd.\n",
      "DOI:  10.1111/cgf.12735\n",
      "ISSN:  14678659\n",
      "citation:  41\n",
      "Impact Factor:  2.078\n",
      "——————————————————————————————————————\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:  Interactive Formation Control in Complex Environments\n",
      "abstract:  The degrees of freedom of a crowd is much higher than that provided by a standard user input device.  Typically, crowd control systems require multiple passes to design crowd movements by specifying waypoints, and then defining character trajectories and crowd formation. Such multi-pass control would spoil the responsiveness and excitement of real-time control systems. In this paper, we propose a single-pass algorithm to control a crowd in complex environments. We observe that low level details in crowd movement are related to interactions between characters and the environment, such as diverging/merging at cross points, or climbing over obstacles. Therefore, we simplify the problem by representing the crowd with a deformable mesh, and allow the user, via multi-touch input, to specify high level movements and formations that are important for context delivery. To help prevent congestion, our system dynamically reassigns characters in the formation by employing a mass transport solver to minimise their overall movement. The solver uses a cost function to evaluate the impact from the environment, including obstacles and areas affecting movement speed. Experimental results show realistic crowd movement created with minimal high-level user inputs.  Our algorithm is particularly useful for real-time applications including strategy games and interactive animation creation.\n",
      "author:  Henry, Joseph and Shum, Hubert P. H. and Komura, Taku\n",
      "year:  2014\n",
      "type:  JOUR\n",
      "journal/booktitle:  IEEE Transactions on Visualization and Computer Graphics\n",
      "volume:  20\n",
      "number of pages:  12\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/TVCG.2013.116\n",
      "ISSN:  10772626\n",
      "citation:  31\n",
      "Impact Factor:  4.579\n",
      "——————————————————————————————————————\n",
      "title:  Environment-aware Real-Time Crowd Control\n",
      "abstract:  Real-time crowd control has become an important research topic due to the recent advancement in console game quality and hardware processing capability. The degrees of freedom of a crowd is much higher than that provided by a standard user input device. As a result most crowd control systems require the user to design the crowd movements through multiple passes, such as first specifying the crowd's start and goal points, then providing the agent trajectories with streamlines. Such a multi-pass control would spoil the responsiveness and excitement of real-time games. In this paper, we propose a new, single-pass algorithm to control crowds using a deformable mesh. When controlling crowds, we observe that most of the low level details are related to passive interactions between the crowd and the environment, such as obstacle avoidance and diverging/merging at cross points. Therefore, we simplify the crowd control problem by representing the crowd with a deformable mesh that passively reacts to the environment. As a result, the user can focus on high level control that is more important for context delivery. Our algorithm provides an efficient crowd control framework while maintaining the quality of the simulation, which is useful for real-time applications such as strategy games.\n",
      "author:  Henry, Joseph and Shum, Hubert P. H. and Komura, Taku\n",
      "year:  2012\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation\n",
      "volume:   \n",
      "number of pages:  8\n",
      "publisher:  Eurographics Association\n",
      "DOI:   \n",
      "ISSN:  9783905674378\n",
      "citation:  51\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Filtering Techniques for Channel Selection in Motor Imagery EEG Applications: A Survey\n",
      "abstract:  Brain Computer Interface (BCI) Systems are used in a wide range of applications such as communication, neuro-prosthetic and environmental control for disabled persons using robots and manipulators. A typical BCI system uses different types of inputs; however, Electroencephalography (EEG) signals are most widely used due to their non-invasive EEG electrodes, portability, and cost efficiency. The signals generated by the brain while performing or imagining a motor related task (Motor Imagery (MI)) signals are one of the important inputs for BCI applications. EEG data is usually recorded from more than 100 locations across the brain, so efficient channel selection algorithms are of great importance to identify optimal channels related to a particular application. The main purpose of applying channel selection is to reduce computational complexity while analysing EEG signals, improve classification accuracy by reducing over-fitting, and decrease setup time. Different channel selection evaluation algorithms such as filtering, wrapper, and hybrid methods have been used for extracting optimal channel subsets by using predefined criteria. After extensively reviewing the literature in the field of EEG channel selection, we can conclude that channel selection algorithms provide a possibility to work with fewer channels without affecting the classification accuracy. In some cases, channel selection increases the system performance by removing the noisy channels. The research in the literature shows that the same performance can be achieved using a smaller channel set, with 10-30 channels in most cases. In this paper, we present a survey of recent development in filtering channel selection techniques along with their feature extraction and classification methods for MI-based EEG applications.\n",
      "author:  Baig, Muhammad Zeeshan and Aslam, Nauman and Shum, Hubert P. H.\n",
      "year:  2020\n",
      "type:  JOUR\n",
      "journal/booktitle:  Artificial Intelligence Review\n",
      "volume:  53\n",
      "number of pages:  26\n",
      "publisher:  Springer\n",
      "DOI:  10.1007/s10462019096948\n",
      "ISSN:   \n",
      "citation:  21\n",
      "Impact Factor:  8.139\n",
      "——————————————————————————————————————\n",
      "title:  Differential Evolution Algorithm as a Tool for Optimal Feature Subset Selection in Motor Imagery EEG\n",
      "abstract:  One of the challenges in developing a Brain Computer Interface (BCI) is dealing with the high dimensionality of the data when extracting features from EEG signals. Different feature selection algorithms have been proposed to overcome this problem but most of them involve complex transformed features, which require high computation and also result in increasing size of the feature set.  In this paper, we present a new hybrid method to select features that involves a Differential Evolution (DE) optimization algorithm for searching the feature space to generate the optimal feature subset, with performance evaluated by a classifier. We provide a comprehensive study of the significance of evolutionary algorithm in selecting the best features for EEG signals. The BCI competition III, dataset IVa has been used to evaluate the method. Experimental results demonstrate that the proposed method performs well with Support Vector Machine (SVM) classifier, with an average classification accuracy of above 95% with a minimum of just 10 features. We also present a comparison of Differential Evolution (DE) with other evolutionary algorithms, and the results show the superiority of DE which implies that, with the selection of a good searching algorithm, a simple Common Spatial Pattern filter features can produce good results.\n",
      "author:  Baig, Muhammad Zeeshan and Aslam, Nauman and Shum, Hubert P. H. and Zhang, Li\n",
      "year:  2017\n",
      "type:  JOUR\n",
      "journal/booktitle:  Expert Systems with Applications\n",
      "volume:  90\n",
      "number of pages:  12\n",
      "publisher:  Elsevier\n",
      "DOI:  10.1016/j.eswa.2017.07.033\n",
      "ISSN:  09574174\n",
      "citation:  80\n",
      "Impact Factor:  6.954\n",
      "——————————————————————————————————————\n",
      "title:  Biofeedback Assessment for Older People with Balance Impairment using a Low-cost Balance Board\n",
      "abstract:  This paper studies the feasibility of using a low-cost game device called Wii Fit Balance Board® to measure the static balance of older people for diagnosing a balance impairment, which is caused by muscle weakness in stroke patients. Sixty participants were invited to attend the risk assessment that included a clinical test. Four biofeedback testing patterns were tested with the participants. Two machine learning algorithms were selected to experiment using 10-fold cross validation scenario. The results show that Artificial Neuron Network has the best evaluation performance of 86.67%, 80%, and 80% in three out of four biofeedback testing patterns. This demonstrates that the application of static balance measurement together with Wii Fit Balance Board® could be implemented as a tool to replace high-cost force plate systems.\n",
      "author:  Rueangsirarak, Worasak and Mekurai, Chayuti and Shum, Hubert P. H. and Uttama, Surapong and Chaisricharoen, Roungsan and Kaewkaen, Kitchana\n",
      "year:  2017\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2017 Global Wireless Summit\n",
      "volume:   \n",
      "number of pages:  6\n",
      "publisher:   \n",
      "DOI:  10.1109/GWS.2017.8300483\n",
      "ISSN:   \n",
      "citation:  2\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:  An Intelligent Mobile-Based Automatic Diagnostic System to Identify Retinal Diseases using Mathematical Morphological Operations\n",
      "abstract:  Diabetic retinopathy is considered in terms of the presence of exudates which cause vision loss in the areas affected. This study targets the development of an intelligent mobile-based automatic diagnosis integrated with a microscopic lens to identify retinal diseases at initial stage at any time or place. Exudate detection is a significant step in order obtaining an early diagnosis of diabetic retinopathy, and if they are segmented accurately, laser treatment can be applied effectively. Consequently, precise segmentation is the fundamental step in exudate extraction. This paper proposes a technique for exudate segmentation in colour retinal images using morphological operations. In this method, after pre-processing, the optic disc and blood vessels are isolated from the retinal image. Exudates are then segmented by a combination of morphological operations such as the modified regionprops function and a reconstruction technique. The proposed technique is verified against the DIARETDB1 database and achieves 85.39% sensitivity. The proposed technique achieves better exudate detection results in terms of sensitivity than other recent methods reported in the literature. In future work, our system will be deployed to a mobile platform to allow efficient and instant diagnosis.\n",
      "author:  Omar, Mohamed and Hossain, Alamgir and Zhang, Li and Shum, Hubert P. H.\n",
      "year:  2014\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2014 International Conference on Software Knowledge Information Management and Applications\n",
      "volume:   \n",
      "number of pages:  5\n",
      "publisher:   \n",
      "DOI:  10.1109/SKIMA.2014.7083563\n",
      "ISSN:   \n",
      "citation:  12\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Human-centric Autonomous Driving in an AV-Pedestrian Interactive Environment Using SVO\n",
      "abstract:  As Autonomous Vehicles (AV) are becoming a reality, the design of efficient motion control algorithms will have to deal with the unpredictable and interactive nature of other road users. Current AV motion planning algorithms suffer from the freezing robot problem, as they often tend to overestimate collision risks. To tackle this problem and design AV that behave human-like, we integrate a concept from Psychology called Social Value Orientation into the Reinforcement Learning (RL) framework. The addition of a social term in the reward function design allows us to tune the AV behaviour towards the pedestrian from a more reckless to an extremely prudent one. We train the vehicle agent with a state of the art RL algorithm and show that Social Value Orientation is an effective tool to obtain pro-social AV behaviour.\n",
      "author:  Crosato, Luca and Wei, Chongfeng and Ho, Edmond S. L. and Shum, Hubert P. H.\n",
      "year:  2021\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2021 IEEE International Conference on HumanMachine Systems\n",
      "volume:   \n",
      "number of pages:  6\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/ICHMS53169.2021.9582640\n",
      "ISSN:   \n",
      "citation:  0\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Multi-task Deep Learning with Optical Flow Features for Self-Driving Cars\n",
      "abstract:  The control of self-driving cars has received growing attention recently. While existing research shows promising results in vehicle control using video from a monocular dash camera, there has been very limited work on directly learning vehicle control from motion-based cues. Such cues are powerful features for visual representations, as they encode the per-pixel movement between two consecutive images, allowing a system to effectively map the features into the control signal. We propose a new framework that exploits the use of a motion-based feature known as optical flow extracted from the dash camera, and demonstrates that such a feature is effective in significantly improving the accuracy of the control signals. Our proposed framework involves two main components. The flow predictor, as a self-supervised deep network, models the underlying scene structure from consecutive frames and generates the optical flow. The controller, as a supervised multi-task deep network, predicts both steer angle and speed. We demonstrate that the proposed framework using the optical flow features can effectively predict control signals from a dash camera video. Using the Cityscapes dataset, we validate that the system prediction has errors as low as 0.0130 rad/s on steer angle and 0.0615 m/s on speed, outperforming existing research.\n",
      "author:  Hu, Yuan and Shum, Hubert P. H. and Ho, Edmond S. L.\n",
      "year:  2020\n",
      "type:  JOUR\n",
      "journal/booktitle:  IET Intelligent Transport Systems\n",
      "volume:  14\n",
      "number of pages:  10\n",
      "publisher:  Institution of Engineering and Technology\n",
      "DOI:  10.1049/ietits.2020.0439\n",
      "ISSN:  1751956X\n",
      "citation:  0\n",
      "Impact Factor:  2.496\n",
      "——————————————————————————————————————\n",
      "title:  A Hybrid Metaheuristic Navigation Algorithm for Robot Path Rolling Planning in an Unknown Environment\n",
      "abstract:  In this paper, a new method for robot path rolling planning in a static and unknown environment based on grid modelling is proposed. In an unknown scene, a local navigation optimization path for the robot is generated intelligently by ant colony optimization (ACO) combined with the environment information of robot’s local view and target information. The robot plans a new navigation path dynamically after certain steps along the previous local navigation path, and always moves along the optimized navigation path which is dynamically modified. The robot will move forward to the target point directly along the local optimization path when the target is within the current view of the robot. This method presents a more intelligent sub-goal mapping method comparing to the traditional rolling window approach. Besides, the path that is part of the generated local path based on the ACO between the current position and the next position of the robot is further optimized using particle swarm optimization (PSO), which resulted in a hybrid metaheuristic algorithm that incorporates ACO and PSO. Simulation results show that the robot can reach the target grid along a global optimization path without collision.\n",
      "author:  Xu, Shoujiang and Ho, Edmond S. L. and Shum, Hubert P. H.\n",
      "year:  2019\n",
      "type:  JOUR\n",
      "journal/booktitle:  Mechatronic Systems and Control\n",
      "volume:  47\n",
      "number of pages:  9\n",
      "publisher:  ACTA Press\n",
      "DOI:  10.2316/J.2019.2013000\n",
      "ISSN:  2561178X\n",
      "citation:  3\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Motion Adaptation for Humanoid Robots in Constrained Environments\n",
      "abstract:  This paper presents a new method to synthesize full body motion for controlling humanoid robots in highly constrained environments. Given a reference motion of the robot and the corresponding environment configuration, the spatial relationships between the robot body parts and the environment objects are extracted as a representation called the Interaction Mesh. Such a representation is then used in adapting the reference motion to an altered environment. By preserving the spatial relationships while satisfying physical constraints, collision-free and well balanced motions can be generated automatically and efficiently. Experimental results show that the proposed method can adapt different full body motions in significantly modified environments. Our method can be applied in precise robotic controls under complicated environments, such as rescue robots in accident scenes and searching robots in highly constrained spaces.\n",
      "author:  Ho, Edmond S. L. and Shum, Hubert P. H.\n",
      "year:  2013\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2013 IEEE International Conference on Robotics and Automation\n",
      "volume:   \n",
      "number of pages:  6\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/ICRA.2013.6631113\n",
      "ISSN:  10504729\n",
      "citation:  31\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Physically-based Character Control in Low Dimensional Space\n",
      "abstract:  In this paper, we propose a new method to compose physically-based character controllers in low dimensional latent space. Source controllers are created by gradually updating the task parameter such as the external force applied to the body. During the optimization, instead of only saving the optimal controllers, we also keep a large number of non-optimal controllers. These controllers provide knowledge about the stable area in the controller space, and are then used as samples to construct a low dimensional manifold that represents stable controllers. During runtime, we interpolate controllers in the low dimensional space and create stable controllers to cope with the irregular external forces. Our method is best to be applied for real-time applications such as computer games.\n",
      "author:  Shum, Hubert P. H. and Komura, Taku and Shiratori, Takaaki and Takagi, Shu\n",
      "year:  2010\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the Third International Conference on Motion in Games\n",
      "volume:  6459\n",
      "number of pages:  12\n",
      "publisher:  SpringerVerlag\n",
      "DOI:  10.1007/9783642169588_3\n",
      "ISSN:  03029743\n",
      "citation:  11\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Curvature-Based Sparse Rule Base Generation for Fuzzy Rule Interpolation\n",
      "abstract:  Fuzzy inference systems have been successfully applied to many real-world applications. Traditional fuzzy inference systems are only applicable to problems with dense rule bases covering the entire problem domains, whilst fuzzy rule interpolation (FRI) works with sparse rule bases that do not cover certain inputs. Thanks to its ability to work with a rule base with less number of rules, FRI approaches have been utilised as a means to reduce system complexity for complex fuzzy models. This is implemented by removing the rules that can be approximated by their neighbours. Most of the existing fuzzy rule base generation and simplification approaches only target dense rule bases for traditional fuzzy inference systems. This paper proposes a new sparse fuzzy rule base generation method to support FRI. In particular, this approach uses curvature values to identify important rules that cannot be accurately approximated by their neighbouring ones for initialising a compact rule base. The initialised rule base is then optimised using an optimisation algorithm by fine-tuning the membership functions of the involved fuzzy sets. Experiments with a simulation model and a real-world application demonstrate the working principle and the actual performance of the proposed system, with results comparable to the traditional methods using rule bases with more rules. \n",
      "author:  Tan, Yao and Shum, Hubert P. H. and Chao, Fei and Vijayakumar, V. and Yang, Longzhi\n",
      "year:  2019\n",
      "type:  JOUR\n",
      "journal/booktitle:  Journal of Intelligent and Fuzzy Systems\n",
      "volume:  36\n",
      "number of pages:  12\n",
      "publisher:  IOS Press\n",
      "DOI:  10.3233/JIFS169978\n",
      "ISSN:   \n",
      "citation:  8\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Manifold Regularized Experimental Design for Active Learning\n",
      "abstract:  Various machine learning and data mining tasks in classification require abundant data samples to be labeled for training. Conventional active learning methods aim at labeling the most informative samples for alleviating the labor of the user. Many previous studies in active learning select one sample after another in a greedy manner. However, this is not very effective because the classification models have to be retrained for each newly labeled sample. Moreover, many popular active learning approaches utilize the most uncertain samples by leveraging the classification hyperplane of the classifier, which is not appropriate since the classification hyperplane is inaccurate when the training data are small-sized. The problem of insufficient training data in real-world systems limits the potential applications of these approaches. This paper presents a novel method of active learning called manifold regularized experimental design (MRED), which can label multiple informative samples at one time for training. In addition, MRED gives an explicit geometric explanation for the selected samples to be labeled by the user. Different from existing active learning methods, our method avoids the intrinsic problems caused by insufficiently labeled samples in real-world applications. Various experiments on synthetic datasets, the Yale face database and the Corel image database have been carried out to show how MRED outperforms existing methods.\n",
      "author:  Zhang, Lining and Shum, Hubert P. H. and Shao, Ling\n",
      "year:  2017\n",
      "type:  JOUR\n",
      "journal/booktitle:  IEEE Transactions on Image Processing\n",
      "volume:  26\n",
      "number of pages:  14\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/TIP.2016.2635440\n",
      "ISSN:  10577149\n",
      "citation:  11\n",
      "Impact Factor:  10.856\n",
      "——————————————————————————————————————\n",
      "title:  Discriminative Semantic Subspace Analysis for Relevance Feedback\n",
      "abstract:  Content-based image retrieval (CBIR) has attracted much attention during the past decades for its potential practical applications to image database management. A variety of relevance feedback (RF) schemes have been designed to bridge the gap between low-level visual features and high-level semantic concepts for an image retrieval task. In the process of RF, it would be impractical or too expensive to provide explicit class label information for each image. Instead, similar or dissimilar pairwise constraints between two images can be acquired more easily. However, most of the conventional RF approaches can only deal with training images with explicit class label information. In this paper, we propose a novel discriminative semantic subspace analysis (DSSA) method, which can directly learn a semantic subspace from similar and dissimilar pairwise constraints without using any explicit class label information. In particular, DSSA can effectively integrate the local geometry of labeled similar images, the discriminative information between labeled similar and dissimilar images, and the local geometry of labeled and unlabeled images together to learn a reliable subspace. Compared with the popular distance metric analysis approaches, our method can also learn a distance metric but perform more effectively when dealing with high-dimensional images. Extensive experiments on both the synthetic data sets and a real-world image database demonstrate the effectiveness of the proposed scheme in improving the performance of the CBIR.\n",
      "author:  Zhang, Lining and Shum, Hubert P. H. and Shao, Ling\n",
      "year:  2016\n",
      "type:  JOUR\n",
      "journal/booktitle:  IEEE Transactions on Image Processing\n",
      "volume:  25\n",
      "number of pages:  13\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/TIP.2016.2516947\n",
      "ISSN:  10577149\n",
      "citation:  29\n",
      "Impact Factor:  10.856\n",
      "——————————————————————————————————————\n",
      "title:  Experience-based Rule Base Generation and Adaptation for Fuzzy Interpolation\n",
      "abstract:  Fuzzy modeling has been widely and successfully applied to solve control problems. Traditional fuzzy modeling requires either complete experts?? knowledge or large data sets to generate rule bases that can fully cover the input domain. Although fuzzy rule interpolation (FRI) relaxes this requirement by approximating rules using their neighboring ones, it is still difficult for some real world applications to obtain sufficient experts?? knowledge and data to generate a reasonable sparse rule base to support FRI. Also, the generated rule bases are usually fixed and ther\n",
      "author:  Li, Jie and Shum, Hubert P. H. and Fu, Xin and Sexton, Graham and Yang, Longzhi\n",
      "year:  2016\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2016 IEEE World Congress on Computational Intelligence\n",
      "volume:   \n",
      "number of pages:  9\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/FUZZIEEE.2016.7737674\n",
      "ISSN:   \n",
      "citation:  14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  Towards Sparse Rule Base Generation for Fuzzy Rule Interpolation\n",
      "abstract:  Fuzzy inference systems have been successfully applied to many real-world applications. Traditional fuzzy inference systems only applicable to problems with dense rule bases by which any observation can be covered; while fuzzy rule interpolation is also able to work with sparse rule bases which may not cover certain observations. Thanks to its ability to work with less rules, fuzzy rule interpolation approaches have also been utilised to reduce system complexity by removing those rules which can be approximated by their neighbouring ones for complex fuzzy models. A number of important fuzzy rule base generation approaches have been proposed in the literature, but the majority of these only target dense rule bases for traditional fuzzy inference systems. This paper proposes a novel sparse fuzzy rule base generation method to support FRI. The approach firstly identifies those important rules which cannot be accurately approximated by their neighbouring ones, to initialise the rule base. Then the raw rule base is optimised by fine tuning the membership functions of the involved fuzzy sets. Digital simulated scenario is employed to demonstrate the working of the proposed system, with promising results generated. \n",
      "author:  Tan, Yao and Li, Jie and Wonders, Martin and Chao, Fei and Shum, Hubert P. H. and Yang, Longzhi\n",
      "year:  2016\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2016 IEEE World Congress on Computational Intelligence\n",
      "volume:   \n",
      "number of pages:  8\n",
      "publisher:  IEEE\n",
      "DOI:  10.1109/FUZZIEEE.2016.7737675\n",
      "ISSN:   \n",
      "citation:  14\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "title:  TSK Inference with Sparse Rule Bases\n",
      "abstract:  The Mamdani and TSK fuzzy models are fuzzy inference engines which have been most widely applied in real-world problems. Compared to the Mamdani approach, the TSK approach is more convenient when the crisp outputs are required. Common to both approaches, when a given observation does not overlap with any rule antecedent in the rule base (which usually termed as a sparse rule base), no rule can be fired, and thus no result can be generated. Fuzzy rule interpolation was proposed to address such issue. Although a number of important fuzzy rule interpolation approaches have been proposed in the literature, all of them were developed for Mamdani inference approach, which leads to the fuzzy outputs. This paper extends the traditional TSK fuzzy inference approach to allow inferences on sparse TSK fuzzy rule bases with crisp outputs directly generated. This extension firstly calculates the similarity degrees between a given observation and every individual rule in the rule base, such that the similarity degrees between the observation and all rule antecedents are greater than 0 even when they do not overlap. Then the TSK fuzzy model is extended using the generated matching degrees to derive crisp inference results. The experimentation shows the promising of the approach in enhancing the TSK inference engine when the knowledge represented in the rule base is not complete.\n",
      "author:  Li, Jie and Qu, Yanpeng and Shum, Hubert P. H. and Yang, Longzhi\n",
      "year:  2016\n",
      "type:  CONF\n",
      "journal/booktitle:  Proceedings of the 2016 UK Workshop on Computational Intelligence\n",
      "volume:   \n",
      "number of pages:  7\n",
      "publisher:  Springer International Publishing\n",
      "DOI:  10.1007/9783319465623_8\n",
      "ISSN:  9783319465623\n",
      "citation:  18\n",
      "Impact Factor:  0\n",
      "——————————————————————————————————————\n",
      "Data Successfully Inserted:\n"
     ]
    }
   ],
   "source": [
    "#build a csv file to store the data and set column names\n",
    "fields=[\"Title\",\"Abstract\",\"Authors\",\"Year\",\"Type\",\"Journal/Booktitle\",\"Volume\",\"Num_Pages\",\"Publisher\",\"DOI\",\"ISSN\",\"Citation\",\"Impact_Factor\"]\n",
    "with open('Text.csv', 'a+', encoding='utf-8_sig', newline='') as file:\n",
    "        csv_writer = csv.writer(file)\n",
    "        csv_writer.writerow(fields)\n",
    "\n",
    "#crawl the text from each publication page(try and except to determining whether a target exists) \n",
    "for url in Filters:\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    publication = soup.find_all('p', attrs={'class': 'TextSmallDefault'})[1] #Avoid duplicate crawling of information in References \n",
    "    publication2 = soup.find_all('p', attrs={'class': 'TextSmallDefault'})[0] #and choose the right format according to needs\n",
    "    \n",
    "    #Title\n",
    "    title = soup.find('h1').text\n",
    "    print('title: ', title)\n",
    "    \n",
    "    #Abstract\n",
    "    abstract = soup.find('p').text\n",
    "    print('abstract: ', abstract)\n",
    "    \n",
    "    #Author(BibTex is more clear than Endnote)\n",
    "    try:\n",
    "        rauthor = publication2.find(text = re.compile('author='))#regular language\n",
    "        formats = re.findall('author={(.+)}',rauthor)#clean the data,removal of unnecessary punctuation\n",
    "        author = formats[0]\n",
    "    except IndexError:\n",
    "        author = ' '\n",
    "    print('author: ', author)\n",
    "    \n",
    "    #Year\n",
    "    try:\n",
    "        year = re.findall('PY(.*?)<', str(publication))[0].strip().replace(' ', '').replace('-', '')#regular language and clean the data\n",
    "    except IndexError:\n",
    "        year = ' '\n",
    "    print('year: ', year)\n",
    "    \n",
    "    #Type\n",
    "    try:\n",
    "        ty=re.findall('TY(.*?)<', str(publication))[0].strip().replace(' ', '').replace('-', '')#same above\n",
    "    except IndexError:\n",
    "        ty = ' '\n",
    "    print('type: ', ty)\n",
    "    \n",
    "    #Journal/Booktitle(based on different type, the article may from journal or book, but they are all T2 in EndNote)\n",
    "    try:\n",
    "        journal = re.findall('T2(.*?)<', str(publication))[0].strip().replace(' ', '').replace('-', '')\n",
    "    except IndexError:\n",
    "        journal = ' '\n",
    "    print('journal/booktitle: ', journal)\n",
    "    \n",
    "    #Volume\n",
    "    try:\n",
    "        vol = re.findall('VL(.*?)<', str(publication))[0].strip().replace(' ', '').replace('-', '')\n",
    "    except IndexError:\n",
    "        vol = ' '\n",
    "    print('volume: ', vol)\n",
    "    \n",
    "    #Number od Pages(Like Author above)\n",
    "    try:\n",
    "        fnumpage = publication2.find(text = re.compile('numpages='))\n",
    "        formats2 = re.findall('numpages={(.+)}',fnumpage)\n",
    "        numpage = formats2[0]\n",
    "    except TypeError:\n",
    "        numpage = ' '\n",
    "    print('number of pages: ', numpage)\n",
    "    \n",
    "    #Publisher\n",
    "    try:\n",
    "        pb = re.findall('PB(.*?)<', str(publication))[0].strip().replace(' ', '').replace('-', '')\n",
    "    except IndexError:\n",
    "        pb = ' '\n",
    "    print('publisher: ', pb)\n",
    "    \n",
    "    #DOI\n",
    "    try:\n",
    "        do = re.findall('DO(.*?)<', str(publication))[0].strip().replace(' ', '').replace('-', '')\n",
    "    except IndexError:\n",
    "        do = ' '\n",
    "    print('DOI: ', do)\n",
    "    \n",
    "    #ISSN\n",
    "    try:\n",
    "        sn = re.findall('SN(.*?)<', str(publication))[0].strip().replace(' ', '').replace('-', '')\n",
    "    except IndexError:\n",
    "        sn = ' '\n",
    "    print('ISSN: ', sn)\n",
    "    \n",
    "    #Citation\n",
    "    try:\n",
    "        citation = re.findall('Citation: (.*?)<', str(page.text))[0]\n",
    "    except IndexError:\n",
    "        citation = '0'\n",
    "    print('citation: ', citation)\n",
    "    \n",
    "    #Impact Factor\n",
    "    try:\n",
    "        impact_factor = re.findall('Impact Factor: (.*?)<', str(page.text))[0]\n",
    "    except IndexError:\n",
    "        impact_factor = '0'\n",
    "    print('Impact Factor: ', impact_factor)\n",
    "\n",
    "    print(\"——————————————————————————————————————\")\n",
    "\n",
    "    row = [title, abstract,author, year, ty, journal, vol, numpage, pb, do, sn, citation, impact_factor]\n",
    "    \n",
    "    #write the data above to the csv file\n",
    "    with open('Text.csv','a+') as file:\n",
    "        csv_writer = csv.writer(file)\n",
    "        csv_writer.writerow(row)\n",
    "    \n",
    "print(\"Data Successfully Inserted:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3 Please design and implement a solution to find out the 100 most popular words used for the title\n",
    "and the abstract of the publications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the csv file to do the analysis\n",
    "df = pd.read_csv(\"Text.csv\", names=fields,\n",
    "                 skiprows = 1, \n",
    "                  index_col=False)\n",
    "#print(df)\n",
    "#get target variables into list\n",
    "Titles=df.Title.to_list()\n",
    "Abstract=df.Abstract.to_list()\n",
    "Authors=df.Authors.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 100 most popular words are \n",
      "motion 149\n",
      "propose 106\n",
      "system 102\n",
      "method 99\n",
      "using 98\n",
      "data 85\n",
      "proposed 73\n",
      "learning 71\n",
      "paper 71\n",
      "human 68\n",
      "features 68\n",
      "results 67\n",
      "new 66\n",
      "applications 65\n",
      "control 60\n",
      "motions 53\n",
      "hand 53\n",
      "3d 52\n",
      "based 52\n",
      "different 51\n",
      "characters 51\n",
      "framework 49\n",
      "network 47\n",
      "methods 47\n",
      "show 47\n",
      "kinect 45\n",
      "interactions 44\n",
      "algorithm 44\n",
      "information 42\n",
      "approach 40\n",
      "environment 40\n",
      "rule 40\n",
      "existing 40\n",
      "images 39\n",
      "body 38\n",
      "depth 38\n",
      "crowd 38\n",
      "problem 38\n",
      "work 37\n",
      "graph 37\n",
      "feature 37\n",
      "applied 37\n",
      "used 37\n",
      "joint 36\n",
      "facial 35\n",
      "real-time 35\n",
      "deep 35\n",
      "games 35\n",
      "movement 35\n",
      "multiple 34\n",
      "model 33\n",
      "experimental 33\n",
      "computer 33\n",
      "performance 33\n",
      "database 33\n",
      "fuzzy 32\n",
      "use 32\n",
      "character 31\n",
      "training 31\n",
      "virtual 31\n",
      "demonstrate 31\n",
      "time 31\n",
      "effectively 31\n",
      "animation 30\n",
      "interaction 30\n",
      "shape 30\n",
      "accuracy 30\n",
      "user 30\n",
      "movements 29\n",
      "real 28\n",
      "pose 28\n",
      "local 28\n",
      "dance 27\n",
      "classification 27\n",
      "analysis 27\n",
      "posture 27\n",
      "action 27\n",
      "quality 27\n",
      "systems 27\n",
      "recognition 26\n",
      "realistic 25\n",
      "research 25\n",
      "postures 25\n",
      "captured 24\n",
      "temporal 24\n",
      "generate 24\n",
      "objects 24\n",
      "representation 23\n",
      "evaluate 23\n",
      "well 23\n",
      "image 22\n",
      "approaches 22\n",
      "poses 22\n",
      "set 22\n",
      "algorithms 22\n",
      "high 21\n",
      "process 21\n",
      "capture 21\n",
      "reality 21\n",
      "complex 21\n"
     ]
    }
   ],
   "source": [
    "#The\"word\" in this solution means one word\n",
    "Text =''.join(Titles + Abstract) #change the list in to string\n",
    "clean = re.sub(r\"\"\"\n",
    "               [,.;@#?!&$]+  \n",
    "               \\ *           \n",
    "               \"\"\",\n",
    "               \" \",          \n",
    "               Text, flags=re.VERBOSE)#Using regular language to clean the punctuation\n",
    "Text_process = clean.lower().split() #split the text into word\n",
    "\n",
    "dic = {} #use dictionary function to count the frequency\n",
    "for word in Text_process:\n",
    "    if word not in dic:\n",
    "        dic[word] = 1\n",
    "    else:\n",
    "        dic[word] = dic[word] + 1\n",
    "\n",
    "swd = sorted(dic.items(),key=operator.itemgetter(1),reverse=True)#sort the data by value\n",
    "\n",
    "stop_words = stopwords.words('english')#using NPL package to delete meaning less words\n",
    "stop_words.extend((\"also\",\"two\",\"due\",\"one\",\"however\")) #based on the results above add more stop words\n",
    "count = 1 #to show 100 word frequency \n",
    "print(\"The 100 most popular words are \")\n",
    "for k,v in swd:\n",
    "    if k not in stop_words and count<=100:\n",
    "        print (k,v)\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5 Please design and implement the solution to use data analysis and visualization for analysing\n",
    "which authors collaborate (or appear) as co-authors in the publications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shum, Hubert P. H. 110\n",
      "Ho, Edmond S. L. 37\n",
      "Komura, Taku 23\n",
      "Aslam, Nauman 14\n",
      "Leung, Howard 11\n",
      "Morishima, Shigeo 11\n",
      "Yang, Longzhi 10\n",
      "Multon, Franck 8\n",
      "Wang, He 7\n",
      "Men, Qianhui 6\n",
      "Plantard, Pierre 6\n",
      "Hu, Shanfeng 6\n",
      "Feng, Qi 6\n",
      "Zhang, Jingtian 5\n",
      "Shao, Ling 5\n",
      "Li, Frederick W. B. 5\n",
      "Liang, Xiaohui 5\n",
      "Iwamoto, Naoya 4\n",
      "Shen, Yijun 4\n",
      "Yamazaki, Shuntaro 4\n",
      "Rueangsirarak, Worasak 4\n",
      "Organisciak, Daniel 4\n",
      "Zhang, Lining 4\n",
      "Chan, Jacky C. P. 3\n",
      "Zhou, Liuyang 3\n",
      "McCay, Kevin D. 3\n",
      "Nozawa, Naoki 3\n",
      "Takagi, Shu 3\n",
      "Bhattacharjee, Debotosh 3\n",
      "Henry, Joseph 3\n",
      "Li, Jie 3\n",
      "Yi, Li 2\n",
      "Asahina, Wakana 2\n",
      "Cheung, Yiu-ming 2\n",
      "Yuen, P. C. 2\n",
      "Hoyet, Ludovic 2\n",
      "Kaewkaen, Kitchana 2\n",
      "Liu, Zhiguang 2\n",
      "Yang, Yang 2\n",
      "Zeng, Lanling 2\n",
      "Tang, Jeff K. T. 2\n",
      "Chen, Jiaying 2\n",
      "Bera, Asish 2\n",
      "Chattopadhyay, Samiran 2\n",
      "Xu, Shoujiang 2\n",
      "Baig, Muhammad Zeeshan 2\n",
      "Zhang, Li 2\n",
      "Tan, Yao 2\n",
      "Chao, Fei 2\n"
     ]
    }
   ],
   "source": [
    "Co_author ='/'.join(Authors) #change the list in to string\n",
    "author_process = Co_author.replace(\" and \",\"/\") #replace 'and' by '/' to prepare for spliting\n",
    "Author_process = author_process.split('/') #split all the names\n",
    "\n",
    "dic2 = {} #count the name\n",
    "for word in Author_process:\n",
    "    if word not in dic2:\n",
    "        dic2[word] = 1\n",
    "    else:\n",
    "        dic2[word] = dic2[word] + 1\n",
    "swd2 = sorted(dic2.items(),key=operator.itemgetter(1),reverse=True) #sort by value\n",
    "\n",
    "for k,v in swd2:\n",
    "    if v > 1:\n",
    "        print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualization\n",
    "x = [i[0] for i in swd2][:10]\n",
    "y = [i[1] for i in swd2][:10]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1]) \n",
    "fig.suptitle('Top 10 co-authors of publications') #title\n",
    "plt.bar(x,y) #use bar chart which is the most suitable visualizations\n",
    "plt.xticks(rotation=45) #make labels can be seen clearly\n",
    "ax.set_xlabel('Author Name') #x-axis text\n",
    "ax.set_ylabel('Number of Publications') #y-axis text\n",
    "for a,b,i in zip(x,y,range(len(x))): \n",
    "    plt.text(a,b+1,y[i],ha='center',fontsize=8) # text the index on each column\n",
    "plt.show()\n",
    "plt.savefig(\"co-author.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5 Please design and implement the solution to use data analysis and visualization for analysing\n",
    "how the features of a publication would affect its “citation”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Title  \\\n",
      "0    Spatio-temporal Manifold Learning for Human Mo...   \n",
      "1    A Quadruple Diffusion Convolutional Recurrent ...   \n",
      "2    GAN-based Reactive Motion Synthesis with Class...   \n",
      "3    A Generic Framework for Editing and Synthesizi...   \n",
      "4    Automatic Sign Dance Synthesis from Gesture-ba...   \n",
      "..                                                 ...   \n",
      "105  Manifold Regularized Experimental Design for A...   \n",
      "106  Discriminative Semantic Subspace Analysis for ...   \n",
      "107  Experience-based Rule Base Generation and Adap...   \n",
      "108  Towards Sparse Rule Base Generation for Fuzzy ...   \n",
      "109               TSK Inference with Sparse Rule Bases   \n",
      "\n",
      "                                              Abstract  \\\n",
      "0    Data-driven modeling of human motions is ubiqu...   \n",
      "1    Recurrent neural network (RNN) has become popu...   \n",
      "2    Creating realistic characters that can react t...   \n",
      "3    Emotion is considered to be a core element in ...   \n",
      "4    Automatic dance synthesis has become more and ...   \n",
      "..                                                 ...   \n",
      "105  Various machine learning and data mining tasks...   \n",
      "106  Content-based image retrieval (CBIR) has attra...   \n",
      "107  Fuzzy modeling has been widely and successfull...   \n",
      "108  Fuzzy inference systems have been successfully...   \n",
      "109  The Mamdani and TSK fuzzy models are fuzzy inf...   \n",
      "\n",
      "                                               Authors  Year  Type  \\\n",
      "0    Wang, He and Ho, Edmond S. L. and Shum, Hubert...  2021  JOUR   \n",
      "1    Men, Qianhui and Ho, Edmond S. L. and Shum, Hu...  2021  JOUR   \n",
      "2    Men, Qianhui and Shum, Hubert P. H. and Ho, Ed...  2021  JOUR   \n",
      "3    Chan, Jacky C. P. and Shum, Hubert P. H. and W...  2019  JOUR   \n",
      "4    Iwamoto, Naoya and Shum, Hubert P. H. and Asah...  2019  CONF   \n",
      "..                                                 ...   ...   ...   \n",
      "105  Zhang, Lining and Shum, Hubert P. H. and Shao,...  2017  JOUR   \n",
      "106  Zhang, Lining and Shum, Hubert P. H. and Shao,...  2016  JOUR   \n",
      "107  Li, Jie and Shum, Hubert P. H. and Fu, Xin and...  2016  CONF   \n",
      "108  Tan, Yao and Li, Jie and Wonders, Martin and C...  2016  CONF   \n",
      "109  Li, Jie and Qu, Yanpeng and Shum, Hubert P. H....  2016  CONF   \n",
      "\n",
      "                                     Journal/Booktitle Volume Num_Pages  \\\n",
      "0    IEEE Transactions on Visualization and Compute...     27        12   \n",
      "1    IEEE Transactions on Circuits and Systems for ...     31        16   \n",
      "2                               Computers and Graphics               12   \n",
      "3                Computer Animation and Virtual Worlds     30        20   \n",
      "4    Proceedings of the 2019 International Conferen...                9   \n",
      "..                                                 ...    ...       ...   \n",
      "105              IEEE Transactions on Image Processing     26        14   \n",
      "106              IEEE Transactions on Image Processing     25        13   \n",
      "107  Proceedings of the 2016 IEEE World Congress on...                9   \n",
      "108  Proceedings of the 2016 IEEE World Congress on...                8   \n",
      "109  Proceedings of the 2016 UK Workshop on Computa...                7   \n",
      "\n",
      "                             Publisher                            DOI  \\\n",
      "0                                 IEEE      10.1109/TVCG.2019.2936810   \n",
      "1                                 IEEE     10.1109/TCSVT.2020.3038145   \n",
      "2                             Elsevier      10.1016/j.cag.2021.09.014   \n",
      "3             John Wiley and Sons Ltd.               10.1002/cav.1871   \n",
      "4                                  ACM        10.1145/3359566.3360069   \n",
      "..                                 ...                            ...   \n",
      "105                               IEEE       10.1109/TIP.2016.2635440   \n",
      "106                               IEEE       10.1109/TIP.2016.2516947   \n",
      "107                               IEEE  10.1109/FUZZIEEE.2016.7737674   \n",
      "108                               IEEE  10.1109/FUZZIEEE.2016.7737675   \n",
      "109  Springer International Publishing        10.1007/9783319465623_8   \n",
      "\n",
      "              ISSN  Citation  Impact_Factor  Video  \n",
      "0                         74          4.579      1  \n",
      "1         10518215        13          4.685      1  \n",
      "2         00978493         0          1.936      1  \n",
      "3                          6          1.020      1  \n",
      "4    9781450369947         0          0.000      1  \n",
      "..             ...       ...            ...    ...  \n",
      "105       10577149        11         10.856      0  \n",
      "106       10577149        29         10.856      0  \n",
      "107                       14          0.000      0  \n",
      "108                       14          0.000      0  \n",
      "109  9783319465623        18          0.000      0  \n",
      "\n",
      "[110 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "#if the publication has a video, 1 as Yes, 0 as No\n",
    "video =[]\n",
    "for url in Filters:\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    if soup.find(\"iframe\")!= None:\n",
    "        video.append(int(1)) #make the result to be intger\n",
    "    else:\n",
    "        video.append(int(0))\n",
    "\n",
    "df['Video'] = video #add the new column to the data frame\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a new data frame for selected features\n",
    "selected_columns = df[[\"Year\",\"Authors\",\"Type\",\"Num_Pages\",\"Publisher\",\"Impact_Factor\",\"Citation\",\"Video\"]]\n",
    "Feature = selected_columns.copy()\n",
    "splitAU = df['Authors'].str.split('and',5, expand=True) #split the name by word'and'\n",
    "splitAU.columns = [\"AU1\", \"AU2\", \"AU3\", \"AU4\",\"AU5\",\"AU6\"] #rename the new column\n",
    "Feature = pd.concat([Feature,splitAU], axis=1) #put the data frame together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Year  Impact_Factor    Citation       Video\n",
      "count   110.000000     110.000000  110.000000  110.000000\n",
      "mean   2016.554545       1.755073   17.254545    0.463636\n",
      "std       4.035590       2.782971   30.982812    0.500958\n",
      "min    2006.000000       0.000000    0.000000    0.000000\n",
      "25%    2014.250000       0.000000    1.000000    0.000000\n",
      "50%    2017.500000       0.000000    6.000000    0.000000\n",
      "75%    2020.000000       2.757000   21.000000    1.000000\n",
      "max    2021.000000      11.448000  212.000000    1.000000\n",
      "                   Year  Impact_Factor  Citation     Video\n",
      "Year           1.000000       0.169173 -0.290235 -0.164647\n",
      "Impact_Factor  0.169173       1.000000  0.458991 -0.097996\n",
      "Citation      -0.290235       0.458991  1.000000  0.180883\n",
      "Video         -0.164647      -0.097996  0.180883  1.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Type</th>\n",
       "      <th>Num_Pages</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Impact_Factor</th>\n",
       "      <th>Citation</th>\n",
       "      <th>Video</th>\n",
       "      <th>AU1</th>\n",
       "      <th>AU2</th>\n",
       "      <th>AU3</th>\n",
       "      <th>AU4</th>\n",
       "      <th>AU5</th>\n",
       "      <th>AU6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2007</td>\n",
       "      <td>Shum, Hubert P. H. and Komura, Taku and Nagano...</td>\n",
       "      <td>CONF</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Shum, Hubert P. H.</td>\n",
       "      <td>Komura, Taku</td>\n",
       "      <td>Nagano, Akinori</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2008</td>\n",
       "      <td>Komura, Taku and Shum, Hubert P. H. and Ho, Ed...</td>\n",
       "      <td>CONF</td>\n",
       "      <td>10</td>\n",
       "      <td>SpringerVerlag</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Komura, Taku</td>\n",
       "      <td>Shum, Hubert P. H.</td>\n",
       "      <td>Ho, Edmond S. L.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2021</td>\n",
       "      <td>Men, Qianhui and Shum, Hubert P. H.</td>\n",
       "      <td>JOUR</td>\n",
       "      <td></td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Men, Qianhui</td>\n",
       "      <td>Shum, Hubert P. H.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2020</td>\n",
       "      <td>Hu, Yuan and Shum, Hubert P. H. and Ho, Edmond...</td>\n",
       "      <td>JOUR</td>\n",
       "      <td>10</td>\n",
       "      <td>Institution of Engineering and Technology</td>\n",
       "      <td>2.496</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hu, Yuan</td>\n",
       "      <td>Shum, Hubert P. H.</td>\n",
       "      <td>Ho, Edmond S. L.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2019</td>\n",
       "      <td>Hall, Jake and Chan, Jacky C. P. and Shum, Hub...</td>\n",
       "      <td>CONF</td>\n",
       "      <td>2</td>\n",
       "      <td>ACM</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hall, Jake</td>\n",
       "      <td>Chan, Jacky C. P.</td>\n",
       "      <td>Shum, Hubert P. H.</td>\n",
       "      <td>Ho, Edmond S. L.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021</td>\n",
       "      <td>Wang, He and Ho, Edmond S. L. and Shum, Hubert...</td>\n",
       "      <td>JOUR</td>\n",
       "      <td>12</td>\n",
       "      <td>IEEE</td>\n",
       "      <td>4.579</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>Wang, He</td>\n",
       "      <td>Ho, Edmond S. L.</td>\n",
       "      <td>Shum, Hubert P. H.</td>\n",
       "      <td>Zhu, Zhanxing</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2017</td>\n",
       "      <td>Baig, Muhammad Zeeshan and Aslam, Nauman and S...</td>\n",
       "      <td>JOUR</td>\n",
       "      <td>12</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>6.954</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>Baig, Muhammad Zeeshan</td>\n",
       "      <td>Aslam, Nauman</td>\n",
       "      <td>Shum, Hubert P. H.</td>\n",
       "      <td>Zhang, Li</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2008</td>\n",
       "      <td>Shum, Hubert P. H. and Komura, Taku and Shirai...</td>\n",
       "      <td>JOUR</td>\n",
       "      <td>8</td>\n",
       "      <td>ACM</td>\n",
       "      <td>5.414</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>Shum, Hubert P. H.</td>\n",
       "      <td>Komura, Taku</td>\n",
       "      <td>Shiraishi, Masashi</td>\n",
       "      <td>Yamazaki, Shuntaro</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2017</td>\n",
       "      <td>Plantard, Pierre and Shum, Hubert P. H. and Pi...</td>\n",
       "      <td>JOUR</td>\n",
       "      <td>8</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>3.661</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>Plantard, Pierre</td>\n",
       "      <td>Shum, Hubert P. H.</td>\n",
       "      <td>Pierres, Anne-Sophie Le</td>\n",
       "      <td>Multon, Franck</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2013</td>\n",
       "      <td>Shum, Hubert P. H. and Ho, Edmond S. L. and Ji...</td>\n",
       "      <td>JOUR</td>\n",
       "      <td>13</td>\n",
       "      <td>IEEE</td>\n",
       "      <td>11.448</td>\n",
       "      <td>212</td>\n",
       "      <td>1</td>\n",
       "      <td>Shum, Hubert P. H.</td>\n",
       "      <td>Ho, Edmond S. L.</td>\n",
       "      <td>Jiang, Yang</td>\n",
       "      <td>Takagi, Shu</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year                                            Authors  Type Num_Pages  \\\n",
       "39   2007  Shum, Hubert P. H. and Komura, Taku and Nagano...  CONF         1   \n",
       "21   2008  Komura, Taku and Shum, Hubert P. H. and Ho, Ed...  CONF        10   \n",
       "87   2021                Men, Qianhui and Shum, Hubert P. H.  JOUR             \n",
       "100  2020  Hu, Yuan and Shum, Hubert P. H. and Ho, Edmond...  JOUR        10   \n",
       "28   2019  Hall, Jake and Chan, Jacky C. P. and Shum, Hub...  CONF         2   \n",
       "..    ...                                                ...   ...       ...   \n",
       "0    2021  Wang, He and Ho, Edmond S. L. and Shum, Hubert...  JOUR        12   \n",
       "96   2017  Baig, Muhammad Zeeshan and Aslam, Nauman and S...  JOUR        12   \n",
       "19   2008  Shum, Hubert P. H. and Komura, Taku and Shirai...  JOUR         8   \n",
       "33   2017  Plantard, Pierre and Shum, Hubert P. H. and Pi...  JOUR         8   \n",
       "52   2013  Shum, Hubert P. H. and Ho, Edmond S. L. and Ji...  JOUR        13   \n",
       "\n",
       "                                     Publisher  Impact_Factor  Citation  \\\n",
       "39                                                      0.000         0   \n",
       "21                              SpringerVerlag          0.000         0   \n",
       "87                                    Elsevier          0.000         0   \n",
       "100  Institution of Engineering and Technology          2.496         0   \n",
       "28                                         ACM          0.000         0   \n",
       "..                                         ...            ...       ...   \n",
       "0                                         IEEE          4.579        74   \n",
       "96                                    Elsevier          6.954        80   \n",
       "19                                         ACM          5.414       125   \n",
       "33                                    Elsevier          3.661       140   \n",
       "52                                        IEEE         11.448       212   \n",
       "\n",
       "     Video                      AU1                   AU2  \\\n",
       "39       1      Shum, Hubert P. H.          Komura, Taku    \n",
       "21       0            Komura, Taku    Shum, Hubert P. H.    \n",
       "87       0            Men, Qianhui     Shum, Hubert P. H.   \n",
       "100      0                Hu, Yuan    Shum, Hubert P. H.    \n",
       "28       0              Hall, Jake     Chan, Jacky C. P.    \n",
       "..     ...                      ...                   ...   \n",
       "0        1                Wang, He      Ho, Edmond S. L.    \n",
       "96       0  Baig, Muhammad Zeeshan         Aslam, Nauman    \n",
       "19       1      Shum, Hubert P. H.          Komura, Taku    \n",
       "33       0        Plantard, Pierre    Shum, Hubert P. H.    \n",
       "52       1      Shum, Hubert P. H.      Ho, Edmond S. L.    \n",
       "\n",
       "                           AU3                  AU4   AU5   AU6  \n",
       "39             Nagano, Akinori                 None  None  None  \n",
       "21            Ho, Edmond S. L.                 None  None  None  \n",
       "87                        None                 None  None  None  \n",
       "100           Ho, Edmond S. L.                 None  None  None  \n",
       "28         Shum, Hubert P. H.      Ho, Edmond S. L.  None  None  \n",
       "..                         ...                  ...   ...   ...  \n",
       "0          Shum, Hubert P. H.         Zhu, Zhanxing  None  None  \n",
       "96         Shum, Hubert P. H.             Zhang, Li  None  None  \n",
       "19         Shiraishi, Masashi    Yamazaki, Shuntaro  None  None  \n",
       "33    Pierres, Anne-Sophie Le        Multon, Franck  None  None  \n",
       "52                Jiang, Yang           Takagi, Shu  None  None  \n",
       "\n",
       "[110 rows x 14 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Feature.describe())#Describe data\n",
    "print(Feature.corr()) #corrlation between Continuous function features\n",
    "Feature.sort_values(by=['Citation']) #sort the table by the number of citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as a new csv file\n",
    "Feature.to_csv(\"Feature.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Figure of Citation and Impact Factor\n",
    "x1 = Feature[\"Citation\"]\n",
    "y1 = Feature[\"Impact_Factor\"]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(x1,y1,alpha=0.3, s=20)\n",
    "fig.suptitle('Scatterplot of Citation and Impact Factor')#Title\n",
    "ax.set_xlabel('Citation') #x-axis text\n",
    "ax.set_ylabel('Impact Factor') #y-axis text\n",
    "#plt.show()\n",
    "plt.savefig(\"Scatterplot of Citation and Impact Factor.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Draft for Citation and Publisher\n",
    "x2 = Feature[\"Citation\"]\n",
    "y2 = Feature[\"Publisher\"]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.scatter(x2,y2,alpha=0.3, s=20)\n",
    "fig.suptitle('Scatterplot of Citation and Publisher')#Title\n",
    "ax.set_xlabel('Citation') #x-axis text\n",
    "ax.set_ylabel('Publisher') #y-axis text\n",
    "#plt.show()\n",
    "plt.savefig(\"Scatterplot of Citation and Publisher.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Boxplot has a more clear version\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "sns.boxplot(x=\"Citation\", y=\"Publisher\", data=Feature)\n",
    "fig.suptitle('Boxplot of Citation and Publisher')#Title\n",
    "ax.set_xlabel('Citation') #x-axis text\n",
    "ax.set_ylabel('Publisher') #y-axis text\n",
    "#plt.show()\n",
    "plt.savefig(\"Boxplot of Citation and Publisher.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f68fa8c7d30>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Draft\n",
    "x3 = Feature[\"Citation\"]\n",
    "y3 = Feature[\"Type\"]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "fig.suptitle('Scatter of Citation and Type')#Title\n",
    "ax.set_xlabel('Citation') #x-axis text\n",
    "ax.set_ylabel('Type') #y-axis text\n",
    "ax.scatter(x3,y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Boxplot has a more clear version\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "sns.boxplot(x=\"Citation\", y=\"Type\", data=Feature)\n",
    "fig.suptitle('Boxplot of Citation and Type')#Title\n",
    "ax.set_xlabel('Citation') #x-axis text\n",
    "ax.set_ylabel('Type') #y-axis text\n",
    "plt.savefig(\"Boxplot of Citation and Type.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lead Author and average citation(Draft)\n",
    "x4 = Feature[\"Citation\"]\n",
    "y4 = Feature[\"AU1\"]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.scatter(x4,y4)\n",
    "plt.yticks(fontsize=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82, 26)\n",
      "(28, 26)\n",
      "(82,)\n",
      "(28,)\n",
      "intercept: 8644.296425711513\n",
      "slope: [-4.29511849e+00  4.04899297e+00  1.23755192e+01  1.70000000e+01\n",
      " -7.10542736e-15  3.62839697e+01 -2.82887261e+01 -5.05412606e+00\n",
      "  6.25914530e+00 -5.73615539e+00  2.75478143e+01 -3.59689405e+00\n",
      " -5.16642453e+01  2.60740092e+00 -7.36155385e-01 -4.84726226e+01\n",
      " -9.32639237e+00 -1.45473233e+01 -2.90012827e+01 -9.10532258e+00\n",
      " -8.88178420e-16 -1.92243295e+01  1.56624588e+01 -1.26743783e+01\n",
      " -5.05412606e+00  0.00000000e+00]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>-4.295118e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Impact_Factor</th>\n",
       "      <td>4.048993e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Video</th>\n",
       "      <td>1.237552e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type_CONF</th>\n",
       "      <td>1.700000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type_GEN</th>\n",
       "      <td>-7.105427e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type_JOUR</th>\n",
       "      <td>3.628397e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type_Preprint</th>\n",
       "      <td>-2.828873e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type_THES</th>\n",
       "      <td>-5.054126e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Publisher_ACM</th>\n",
       "      <td>6.259145e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Publisher_ACTA Press</th>\n",
       "      <td>-5.736155e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Publisher_Academic Press</th>\n",
       "      <td>2.754781e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Publisher_Elsevier</th>\n",
       "      <td>-3.596894e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Publisher_Eurographics Association</th>\n",
       "      <td>-5.166425e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Publisher_IEEE</th>\n",
       "      <td>2.607401e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Publisher_IOS Press</th>\n",
       "      <td>-7.361554e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Publisher_IPI Press</th>\n",
       "      <td>-4.847262e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Publisher_Inderscience</th>\n",
       "      <td>-9.326392e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Publisher_Institution of Engineering and Technology</th>\n",
       "      <td>-1.454732e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Publisher_John Wiley and Sons Ltd.</th>\n",
       "      <td>-2.900128e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Publisher_SAGE</th>\n",
       "      <td>-9.105323e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Publisher_SciTePress</th>\n",
       "      <td>-8.881784e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Publisher_Springer</th>\n",
       "      <td>-1.922433e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Publisher_Springer International Publishing</th>\n",
       "      <td>1.566246e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Publisher_SpringerVerlag</th>\n",
       "      <td>-1.267438e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Publisher_University of Edinburgh</th>\n",
       "      <td>-5.054126e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Publisher_WO Patent WO/2010/057897</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Coefficient\n",
       "Year                                               -4.295118e+00\n",
       "Impact_Factor                                       4.048993e+00\n",
       "Video                                               1.237552e+01\n",
       "Type_CONF                                           1.700000e+01\n",
       "Type_GEN                                           -7.105427e-15\n",
       "Type_JOUR                                           3.628397e+01\n",
       "Type_Preprint                                      -2.828873e+01\n",
       "Type_THES                                          -5.054126e+00\n",
       "Publisher_ACM                                       6.259145e+00\n",
       "Publisher_ACTA Press                               -5.736155e+00\n",
       "Publisher_Academic Press                            2.754781e+01\n",
       "Publisher_Elsevier                                 -3.596894e+00\n",
       "Publisher_Eurographics Association                 -5.166425e+01\n",
       "Publisher_IEEE                                      2.607401e+00\n",
       "Publisher_IOS Press                                -7.361554e-01\n",
       "Publisher_IPI Press                                -4.847262e+01\n",
       "Publisher_Inderscience                             -9.326392e+00\n",
       "Publisher_Institution of Engineering and Techno... -1.454732e+01\n",
       "Publisher_John Wiley and Sons Ltd.                 -2.900128e+01\n",
       "Publisher_SAGE                                     -9.105323e+00\n",
       "Publisher_SciTePress                               -8.881784e-16\n",
       "Publisher_Springer                                 -1.922433e+01\n",
       "Publisher_Springer International Publishing         1.566246e+01\n",
       "Publisher_SpringerVerlag                           -1.267438e+01\n",
       "Publisher_University of Edinburgh                  -5.054126e+00\n",
       "Publisher_WO Patent WO/2010/057897                  0.000000e+00"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Linear Regression\n",
    "X=Feature[[\"Year\",\"Type\",\"Publisher\",\"Impact_Factor\",\"Video\"]]\n",
    "X = pd.get_dummies(data=X, drop_first=True)\n",
    "Y = Feature[\"Citation\"]\n",
    "cols = [\"Year\",\"Type\",\"Publisher\",\"Impact_Factor\",\"Video\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=101) #split the data\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train,y_train) #model building\n",
    "print('intercept:', model.intercept_) #coeffient\n",
    "print('slope:', model.coef_)\n",
    "zip(cols,model.coef_)\n",
    "coeff_parameter = pd.DataFrame(model.coef_,X.columns,columns=['Coefficient'])\n",
    "coeff_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.25107701681509\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(metrics.mean_absolute_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82, 5)\n",
      "(28, 5)\n",
      "(82,)\n",
      "(28,)\n",
      "intercept: 1.4210854715202004e-14\n",
      "slope: [9.83333333e+00 0.00000000e+00 3.08108108e+01 9.88098492e-15\n",
      " 1.00000000e+00]\n",
      "16.022522522522525\n"
     ]
    }
   ],
   "source": [
    "#Linear Regression Model2  To test weather the number of feature effect the accuracy\n",
    "X=Feature[[\"Type\"]]\n",
    "X = pd.get_dummies(data=X, drop_first=True)\n",
    "Y = Feature[\"Citation\"]\n",
    "cols2 = [\"Type\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=19)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "model2 = LinearRegression()\n",
    "model2.fit(X_train,y_train)\n",
    "print('intercept:', model2.intercept_)\n",
    "print('slope:', model2.coef_)\n",
    "zip(cols2,model2.coef_)\n",
    "y_pred = model2.predict(X_test)\n",
    "print(metrics.mean_absolute_error(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
